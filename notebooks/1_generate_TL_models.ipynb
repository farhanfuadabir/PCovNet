{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: e:\\Repositories\\PCovNet\\src\n",
      "Found GPU at: /device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "# Default packages\n",
    "import os\n",
    "from os.path import join\n",
    "from datetime import date, datetime, timedelta\n",
    "from joblib import load, dump\n",
    "\n",
    "# Set current directory to \"src\"\n",
    "os.chdir(join(os.getcwd(), os.pardir, \"src\"))\n",
    "print(f\"Current working directory: {os.getcwd()}\")\n",
    "\n",
    "# Installed packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.backend import clear_session\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "# Defined modules\n",
    "from util.util import *\n",
    "# from config.config import *\n",
    "from process.dataloader import DataTL\n",
    "from process.embed_gen import EmbedGenTL\n",
    "from process.evaluate import *\n",
    "from models import lstm_vae, cnn_vae\n",
    "from models.lstm_ae import lstm_autoencoder\n",
    "from models.lstm import basic_lstm\n",
    "from visualize import plot\n",
    "\n",
    "# Set TF log level to minimum\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"\n",
    "\n",
    "# Check GPU availability\n",
    "device_name = tf.test.gpu_device_name()\n",
    "if device_name != '/device:GPU:0':\n",
    "    raise SystemError('GPU device not found')\n",
    "print('Found GPU at: {}'.format(device_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    'EXP_NAME': \"trained_model_healthy_phase1\",\n",
    "    'EXP_PHASE': \"phase1\",\n",
    "    'EXP_GROUP': \"healthy\",\n",
    "    'AUGMENT_DATA': True,\n",
    "    'LEN_WIN': 24,\n",
    "    'N_WIN': 14,\n",
    "    'LATENT_DIM': 6,\n",
    "    'BATCH_SIZE': 64,\n",
    "    'VAL_SPLIT': 0.05,\n",
    "    'LEARNING_RATE': 0.0002,\n",
    "    'EPOCH': 1000,\n",
    "    'PATIENCE': 10,\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "config\n",
      "{\n",
      "    \"EXP_NAME\": \"trained_model_healthy_phase1\",\n",
      "    \"EXP_PHASE\": \"phase1\",\n",
      "    \"EXP_GROUP\": \"healthy\",\n",
      "    \"AUGMENT_DATA\": true,\n",
      "    \"LEN_WIN\": 24,\n",
      "    \"N_WIN\": 14,\n",
      "    \"LATENT_DIM\": 6,\n",
      "    \"BATCH_SIZE\": 64,\n",
      "    \"VAL_SPLIT\": 0.05,\n",
      "    \"LEARNING_RATE\": 0.0002,\n",
      "    \"EPOCH\": 1000,\n",
      "    \"PATIENCE\": 10,\n",
      "    \"EXP_DIR\": \"e:\\\\Repositories\\\\PCovNet\\\\src\\\\..\\\\experiment\\\\trained_model_healthy_phase1_24_14\",\n",
      "    \"DATA_DIR\": \"e:\\\\Repositories\\\\PCovNet\\\\src\\\\..\\\\data\\\\raw\\\\phase1\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Dictionaries of dataset paths\n",
    "DATA_DIR_DICT = {\n",
    "    'phase1': join(os.getcwd(), os.pardir, \"data\", \"raw\", \"phase1\"),\n",
    "    'phase2': join(os.getcwd(), os.pardir, \"data\", \"raw\", \"phase2\")\n",
    "}\n",
    "INFO_DIR_DICT = {\n",
    "    'covid_phase1': join(os.getcwd(), os.pardir, \"data\", \"external\", \"covid_phase1_info.csv\"),\n",
    "    'covid_phase2': join(os.getcwd(), os.pardir, \"data\", \"external\", \"covid_phase2_info.csv\"),\n",
    "    'healthy_phase1': join(os.getcwd(), os.pardir, \"data\", \"external\", \"healthy_phase1_info.csv\"),\n",
    "    'non-covid_phase1': join(os.getcwd(), os.pardir, \"data\", \"external\", \"non-covid_phase1_info.csv\"),\n",
    "}\n",
    "\n",
    "# Import subject info\n",
    "subject_info = pd.read_csv(\n",
    "    INFO_DIR_DICT[f\"{config['EXP_GROUP']}_{config['EXP_PHASE']}\"])\n",
    "\n",
    "\n",
    "# Assign experiment directory\n",
    "config['EXP_DIR'] = join(os.getcwd(), os.pardir, \"experiment\",\n",
    "                         f\"{config['EXP_NAME']}_{config['LEN_WIN']}_{config['N_WIN']}\")\n",
    "# timestamp = datetime.now().strftime(f\"%Y-%m-%d %H-%M__\")\n",
    "# config['EXP_DIR'] = join(os.getcwd(), os.pardir, \"experiment\",\n",
    "#                          timestamp + config['EXP_NAME'])\n",
    "handle_dir(config['EXP_DIR'])\n",
    "\n",
    "\n",
    "# Add DATA_DIR\n",
    "config['DATA_DIR'] = DATA_DIR_DICT[config['EXP_PHASE']]\n",
    "\n",
    "\n",
    "# Export config\n",
    "export_json(config, join(config['EXP_DIR'], \"config.json\"),\n",
    "            print_json=True)\n",
    "\n",
    "# Start logging\n",
    "with open(join(config['EXP_DIR'], \"log.txt\"), 'w', encoding='utf-8') as f:\n",
    "    for key, value in config.items():\n",
    "        f.write(f\"{key}: {value}\\n\")\n",
    "    f.write(\"\\n\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import & Process Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 63/63 [01:43<00:00,  1.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            Subject Info\n",
      "            ============\n",
      "            Phase:                phase1\n",
      "            Group:                healthy\n",
      "            \n",
      "            Dataset Shape\n",
      "            =============\n",
      "            VAE Train:            (55208, 24, 1)\n",
      "            VAE Train Aug:        (441664, 24, 1)\n",
      "            LSTM Train:           (35739, 14, 24, 1)\n",
      "        \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "if 'Symptom Onset' in subject_info.columns:\n",
    "    subject_info.drop(['Symptom Onset'], axis=1)\n",
    "\n",
    "data_obj_path = join(config['EXP_DIR'],\n",
    "                     f\"{config['LEN_WIN']}-{config['N_WIN']}_data.joblib\")\n",
    "\n",
    "if not os.path.isfile(data_obj_path):\n",
    "    # Prepare data\n",
    "    data = DataTL(config, subject_info)\n",
    "\n",
    "    # Print data info\n",
    "    data.print_info()\n",
    "\n",
    "    # Export data object\n",
    "    dump(data, data_obj_path)\n",
    "else:\n",
    "    data = load(data_obj_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VAE Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Assign and Compile VAE Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "VAE Model Summary\n",
      "=================\n",
      "\n",
      "Model: \"Encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " encoder_input (InputLayer)     [(None, 24, 1)]      0           []                               \n",
      "                                                                                                  \n",
      " encoder1 (Conv1D)              (None, 12, 128)      512         ['encoder_input[0][0]']          \n",
      "                                                                                                  \n",
      " encoder2 (Conv1D)              (None, 6, 64)        24640       ['encoder1[0][0]']               \n",
      "                                                                                                  \n",
      " encoder3 (Conv1D)              (None, 3, 32)        6176        ['encoder2[0][0]']               \n",
      "                                                                                                  \n",
      " flatten (Flatten)              (None, 96)           0           ['encoder3[0][0]']               \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 6)            582         ['flatten[0][0]']                \n",
      "                                                                                                  \n",
      " z_mean (Dense)                 (None, 6)            42          ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      " z_var (Dense)                  (None, 6)            42          ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      " z (Sampling)                   (None, 6)            0           ['z_mean[0][0]',                 \n",
      "                                                                  'z_var[0][0]']                  \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 31,994\n",
      "Trainable params: 31,994\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "\n",
      "\n",
      "Model: \"Decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 6)]               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 96)                672       \n",
      "                                                                 \n",
      " reshape (Reshape)           (None, 3, 32)             0         \n",
      "                                                                 \n",
      " decoder1 (Conv1DTranspose)  (None, 6, 32)             3104      \n",
      "                                                                 \n",
      " decoder2 (Conv1DTranspose)  (None, 12, 64)            6208      \n",
      "                                                                 \n",
      " decoder3 (Conv1DTranspose)  (None, 24, 128)           24704     \n",
      "                                                                 \n",
      " conv1d_transpose (Conv1DTra  (None, 24, 1)            385       \n",
      " nspose)                                                         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 35,073\n",
      "Trainable params: 35,073\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "\n",
      "Model: \"Variational_AutoEncoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " Input (InputLayer)          [(None, 24, 1)]           0         \n",
      "                                                                 \n",
      " Encoder (Functional)        [(None, 6),               31994     \n",
      "                              (None, 6),                         \n",
      "                              (None, 6)]                         \n",
      "                                                                 \n",
      " Decoder (Functional)        (None, 24, 1)             35073     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 67,067\n",
      "Trainable params: 67,067\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Get VAE model\n",
    "vae_model = cnn_vae.VAE(n_timesteps=config['LEN_WIN'],\n",
    "                        n_channels=data.train_dataset_vae.shape[-1],\n",
    "                        latent_dim=config['LATENT_DIM'])\n",
    "vae_model.compile(loss=tf.losses.MeanSquaredError(),\n",
    "                  optimizer=tf.optimizers.Adam(\n",
    "                      learning_rate=config['LEARNING_RATE']),\n",
    "                  metrics=[tf.metrics.MeanSquaredError()])\n",
    "\n",
    "# Show VAE model summary\n",
    "print(\"\\nVAE Model Summary\")\n",
    "print(\"=================\", end=\"\\n\\n\")\n",
    "vae_model.print_summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train VAE Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "820/820 [==============================] - 114s 132ms/step - loss: 22.9381 - reconst_loss: 22.1067 - kl_loss: 0.8314 - val_loss: 19.3333 - val_reconst_loss: 17.5032 - val_kl_loss: 1.8301\n",
      "Epoch 2/1000\n",
      "820/820 [==============================] - 54s 66ms/step - loss: 16.6016 - reconst_loss: 15.1213 - kl_loss: 1.4803 - val_loss: 16.3789 - val_reconst_loss: 14.8294 - val_kl_loss: 1.5496\n",
      "Epoch 3/1000\n",
      "820/820 [==============================] - 42s 51ms/step - loss: 15.9902 - reconst_loss: 14.6892 - kl_loss: 1.3010 - val_loss: 15.9997 - val_reconst_loss: 14.8071 - val_kl_loss: 1.1926\n",
      "Epoch 4/1000\n",
      "820/820 [==============================] - 43s 53ms/step - loss: 15.7754 - reconst_loss: 14.5808 - kl_loss: 1.1946 - val_loss: 15.8346 - val_reconst_loss: 14.5868 - val_kl_loss: 1.2477\n",
      "Epoch 5/1000\n",
      "820/820 [==============================] - 46s 57ms/step - loss: 15.6060 - reconst_loss: 14.4672 - kl_loss: 1.1388 - val_loss: 15.6191 - val_reconst_loss: 14.4894 - val_kl_loss: 1.1297\n",
      "Epoch 6/1000\n",
      "820/820 [==============================] - 40s 48ms/step - loss: 15.4471 - reconst_loss: 14.3716 - kl_loss: 1.0755 - val_loss: 15.6119 - val_reconst_loss: 14.3492 - val_kl_loss: 1.2627\n",
      "Epoch 7/1000\n",
      "820/820 [==============================] - 40s 49ms/step - loss: 15.3333 - reconst_loss: 14.2936 - kl_loss: 1.0397 - val_loss: 15.4093 - val_reconst_loss: 14.3650 - val_kl_loss: 1.0443\n",
      "Epoch 8/1000\n",
      "820/820 [==============================] - 44s 54ms/step - loss: 15.2576 - reconst_loss: 14.2052 - kl_loss: 1.0523 - val_loss: 15.4296 - val_reconst_loss: 14.3647 - val_kl_loss: 1.0649\n",
      "Epoch 9/1000\n",
      "820/820 [==============================] - 43s 53ms/step - loss: 15.2206 - reconst_loss: 14.1492 - kl_loss: 1.0714 - val_loss: 15.2664 - val_reconst_loss: 14.2315 - val_kl_loss: 1.0349\n",
      "Epoch 10/1000\n",
      "820/820 [==============================] - 46s 56ms/step - loss: 15.1971 - reconst_loss: 14.1175 - kl_loss: 1.0795 - val_loss: 15.3489 - val_reconst_loss: 14.2121 - val_kl_loss: 1.1368\n",
      "Epoch 11/1000\n",
      "820/820 [==============================] - 44s 53ms/step - loss: 15.1599 - reconst_loss: 14.0786 - kl_loss: 1.0813 - val_loss: 15.4171 - val_reconst_loss: 14.3027 - val_kl_loss: 1.1144\n",
      "Epoch 12/1000\n",
      "820/820 [==============================] - 42s 51ms/step - loss: 15.1673 - reconst_loss: 14.0648 - kl_loss: 1.1025 - val_loss: 15.2855 - val_reconst_loss: 14.1328 - val_kl_loss: 1.1528\n",
      "Epoch 13/1000\n",
      "820/820 [==============================] - 48s 58ms/step - loss: 15.1388 - reconst_loss: 14.0317 - kl_loss: 1.1071 - val_loss: 15.2155 - val_reconst_loss: 14.1414 - val_kl_loss: 1.0741\n",
      "Epoch 14/1000\n",
      "820/820 [==============================] - 35s 43ms/step - loss: 15.1317 - reconst_loss: 14.0172 - kl_loss: 1.1144 - val_loss: 15.2581 - val_reconst_loss: 14.0704 - val_kl_loss: 1.1877\n",
      "Epoch 15/1000\n",
      "820/820 [==============================] - 35s 43ms/step - loss: 15.1147 - reconst_loss: 13.9936 - kl_loss: 1.1211 - val_loss: 15.4459 - val_reconst_loss: 14.2809 - val_kl_loss: 1.1650\n",
      "Epoch 16/1000\n",
      "820/820 [==============================] - 41s 50ms/step - loss: 15.0995 - reconst_loss: 13.9843 - kl_loss: 1.1152 - val_loss: 15.2761 - val_reconst_loss: 14.0889 - val_kl_loss: 1.1871\n",
      "Epoch 17/1000\n",
      "820/820 [==============================] - 34s 41ms/step - loss: 15.0902 - reconst_loss: 13.9585 - kl_loss: 1.1317 - val_loss: 15.1934 - val_reconst_loss: 14.0628 - val_kl_loss: 1.1306\n",
      "Epoch 18/1000\n",
      "820/820 [==============================] - 48s 59ms/step - loss: 15.0776 - reconst_loss: 13.9462 - kl_loss: 1.1315 - val_loss: 15.2333 - val_reconst_loss: 14.0285 - val_kl_loss: 1.2048\n",
      "Epoch 19/1000\n",
      "820/820 [==============================] - 48s 59ms/step - loss: 15.0831 - reconst_loss: 13.9361 - kl_loss: 1.1471 - val_loss: 15.1935 - val_reconst_loss: 13.9904 - val_kl_loss: 1.2031\n",
      "Epoch 20/1000\n",
      "820/820 [==============================] - 40s 49ms/step - loss: 15.0669 - reconst_loss: 13.9198 - kl_loss: 1.1472 - val_loss: 15.2043 - val_reconst_loss: 14.0899 - val_kl_loss: 1.1144\n",
      "Epoch 21/1000\n",
      "820/820 [==============================] - 45s 55ms/step - loss: 15.0792 - reconst_loss: 13.9210 - kl_loss: 1.1582 - val_loss: 15.1781 - val_reconst_loss: 13.9977 - val_kl_loss: 1.1804\n",
      "Epoch 22/1000\n",
      "820/820 [==============================] - 32s 39ms/step - loss: 15.0624 - reconst_loss: 13.9067 - kl_loss: 1.1558 - val_loss: 15.1604 - val_reconst_loss: 13.9919 - val_kl_loss: 1.1685\n",
      "Epoch 23/1000\n",
      "820/820 [==============================] - 35s 43ms/step - loss: 15.0694 - reconst_loss: 13.8995 - kl_loss: 1.1699 - val_loss: 15.2215 - val_reconst_loss: 14.0506 - val_kl_loss: 1.1709\n",
      "Epoch 24/1000\n",
      "820/820 [==============================] - 41s 51ms/step - loss: 15.0480 - reconst_loss: 13.8870 - kl_loss: 1.1610 - val_loss: 15.1915 - val_reconst_loss: 14.0720 - val_kl_loss: 1.1195\n",
      "Epoch 25/1000\n",
      "820/820 [==============================] - 43s 52ms/step - loss: 15.0505 - reconst_loss: 13.8867 - kl_loss: 1.1638 - val_loss: 15.1491 - val_reconst_loss: 14.0607 - val_kl_loss: 1.0884\n",
      "Epoch 26/1000\n",
      "820/820 [==============================] - 36s 44ms/step - loss: 15.0406 - reconst_loss: 13.8744 - kl_loss: 1.1662 - val_loss: 15.1436 - val_reconst_loss: 13.9565 - val_kl_loss: 1.1871\n",
      "Epoch 27/1000\n",
      "820/820 [==============================] - 38s 46ms/step - loss: 15.0433 - reconst_loss: 13.8743 - kl_loss: 1.1690 - val_loss: 15.1488 - val_reconst_loss: 13.9926 - val_kl_loss: 1.1562\n",
      "Epoch 28/1000\n",
      "820/820 [==============================] - 40s 48ms/step - loss: 15.0461 - reconst_loss: 13.8630 - kl_loss: 1.1832 - val_loss: 15.2143 - val_reconst_loss: 14.0509 - val_kl_loss: 1.1634\n",
      "Epoch 29/1000\n",
      "820/820 [==============================] - 43s 53ms/step - loss: 15.0356 - reconst_loss: 13.8515 - kl_loss: 1.1841 - val_loss: 15.1359 - val_reconst_loss: 13.9121 - val_kl_loss: 1.2238\n",
      "Epoch 30/1000\n",
      "820/820 [==============================] - 35s 43ms/step - loss: 15.0422 - reconst_loss: 13.8590 - kl_loss: 1.1832 - val_loss: 15.1207 - val_reconst_loss: 13.9821 - val_kl_loss: 1.1386\n",
      "Epoch 31/1000\n",
      "820/820 [==============================] - 42s 51ms/step - loss: 15.0278 - reconst_loss: 13.8469 - kl_loss: 1.1809 - val_loss: 15.1732 - val_reconst_loss: 14.0396 - val_kl_loss: 1.1336\n",
      "Epoch 32/1000\n",
      "820/820 [==============================] - 35s 43ms/step - loss: 15.0272 - reconst_loss: 13.8448 - kl_loss: 1.1824 - val_loss: 15.1896 - val_reconst_loss: 14.0007 - val_kl_loss: 1.1890\n",
      "Epoch 33/1000\n",
      "820/820 [==============================] - 35s 42ms/step - loss: 15.0283 - reconst_loss: 13.8490 - kl_loss: 1.1793 - val_loss: 15.2633 - val_reconst_loss: 14.0224 - val_kl_loss: 1.2409\n",
      "Epoch 34/1000\n",
      "820/820 [==============================] - 40s 49ms/step - loss: 15.0299 - reconst_loss: 13.8425 - kl_loss: 1.1874 - val_loss: 15.2048 - val_reconst_loss: 14.0519 - val_kl_loss: 1.1528\n",
      "Epoch 35/1000\n",
      "820/820 [==============================] - 33s 40ms/step - loss: 15.0286 - reconst_loss: 13.8491 - kl_loss: 1.1795 - val_loss: 15.1903 - val_reconst_loss: 14.0669 - val_kl_loss: 1.1234\n",
      "Epoch 36/1000\n",
      "820/820 [==============================] - 35s 42ms/step - loss: 15.0355 - reconst_loss: 13.8431 - kl_loss: 1.1924 - val_loss: 15.0832 - val_reconst_loss: 13.9382 - val_kl_loss: 1.1450\n",
      "Epoch 37/1000\n",
      "820/820 [==============================] - 34s 42ms/step - loss: 15.0179 - reconst_loss: 13.8328 - kl_loss: 1.1851 - val_loss: 15.1595 - val_reconst_loss: 13.8609 - val_kl_loss: 1.2986\n",
      "Epoch 38/1000\n",
      "820/820 [==============================] - 34s 41ms/step - loss: 15.0215 - reconst_loss: 13.8279 - kl_loss: 1.1936 - val_loss: 15.1875 - val_reconst_loss: 13.8787 - val_kl_loss: 1.3088\n",
      "Epoch 39/1000\n",
      "820/820 [==============================] - 30s 37ms/step - loss: 15.0117 - reconst_loss: 13.8327 - kl_loss: 1.1790 - val_loss: 15.1547 - val_reconst_loss: 13.9291 - val_kl_loss: 1.2255\n",
      "Epoch 40/1000\n",
      "820/820 [==============================] - 30s 37ms/step - loss: 15.0171 - reconst_loss: 13.8359 - kl_loss: 1.1812 - val_loss: 15.1869 - val_reconst_loss: 14.0240 - val_kl_loss: 1.1629\n",
      "Epoch 41/1000\n",
      "820/820 [==============================] - 35s 43ms/step - loss: 15.0191 - reconst_loss: 13.8300 - kl_loss: 1.1891 - val_loss: 15.2373 - val_reconst_loss: 13.8962 - val_kl_loss: 1.3411\n",
      "Epoch 42/1000\n",
      "820/820 [==============================] - 31s 38ms/step - loss: 15.0269 - reconst_loss: 13.8318 - kl_loss: 1.1952 - val_loss: 15.2361 - val_reconst_loss: 13.9734 - val_kl_loss: 1.2627\n",
      "Epoch 43/1000\n",
      "820/820 [==============================] - 28s 35ms/step - loss: 15.0133 - reconst_loss: 13.8280 - kl_loss: 1.1852 - val_loss: 15.1329 - val_reconst_loss: 13.8702 - val_kl_loss: 1.2627\n",
      "Epoch 44/1000\n",
      "820/820 [==============================] - 36s 43ms/step - loss: 15.0121 - reconst_loss: 13.8261 - kl_loss: 1.1860 - val_loss: 15.1403 - val_reconst_loss: 13.8280 - val_kl_loss: 1.3123\n",
      "Epoch 45/1000\n",
      "820/820 [==============================] - 33s 40ms/step - loss: 15.0090 - reconst_loss: 13.8192 - kl_loss: 1.1897 - val_loss: 15.1363 - val_reconst_loss: 13.9883 - val_kl_loss: 1.1480\n",
      "Epoch 46/1000\n",
      "820/820 [==============================] - 33s 40ms/step - loss: 15.0094 - reconst_loss: 13.8272 - kl_loss: 1.1821 - val_loss: 15.1362 - val_reconst_loss: 13.8960 - val_kl_loss: 1.2402\n"
     ]
    }
   ],
   "source": [
    "# Assign checkpoint paths\n",
    "vae_ckpt_path = join(\n",
    "    config['EXP_DIR'], \"vae_checkpoint\", \"ckpt\")\n",
    "\n",
    "# Callbacks for VAE\n",
    "early_stopping_callback = EarlyStopping(monitor='val_loss',\n",
    "                                        patience=config['PATIENCE'],\n",
    "                                        mode='min',\n",
    "                                        restore_best_weights=True)\n",
    "checkpoint_callback = ModelCheckpoint(vae_ckpt_path,\n",
    "                                      monitor='val_loss',\n",
    "                                      verbose=0,\n",
    "                                      mode='min',\n",
    "                                      save_best_only=True,\n",
    "                                      save_weights_only=True)\n",
    "\n",
    "if not os.path.isfile(vae_ckpt_path + \".index\"):\n",
    "    # Train VAE\n",
    "    vae_history = vae_model.fit(data.train_dataset_vae,\n",
    "                                validation_split=config['VAL_SPLIT'],\n",
    "                                batch_size=config['BATCH_SIZE'],\n",
    "                                epochs=config['EPOCH'],\n",
    "                                shuffle=False,\n",
    "                                verbose=1,\n",
    "                                callbacks=[early_stopping_callback, checkpoint_callback])\n",
    "\n",
    "    # Export model history\n",
    "    export_history(vae_history, join(\n",
    "        config['EXP_DIR'], \"vae_history.csv\"))\n",
    "\n",
    "    # Plot loss curve\n",
    "    # print(\"\\nVAE Loss Curve\")\n",
    "    # print(\"==============\", end=\"\\n\")\n",
    "    plot.loss_curve(config, vae_history, ref=\"_VAE\", save_plot=True,\n",
    "                    close_plot=True)\n",
    "\n",
    "else:\n",
    "    vae_model.load_weights(vae_ckpt_path)\n",
    "    print(\"VAE model weights loaded from:\")\n",
    "    print(vae_ckpt_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating embeddings for train dataset... \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 35739/35739 [08:31<00:00, 69.82it/s] \n"
     ]
    }
   ],
   "source": [
    "if not os.path.isfile(join(config['EXP_DIR'], f\"vae_embeddings.joblib\")):\n",
    "    # Get embedding dataset\n",
    "    embed_gen = EmbedGenTL(config, vae_model,\n",
    "                           data, verbose=True)\n",
    "\n",
    "    # Save embed_gen object\n",
    "    dump(embed_gen, join(config['EXP_DIR'],\n",
    "                         f\"vae_embeddings.joblib\"))\n",
    "else:\n",
    "    # Load embed_gen object\n",
    "    embed_gen = load(join(config['EXP_DIR'], f\"vae_embeddings.joblib\"))\n",
    "    print(\"Embeddings loaded from:\")\n",
    "    print(join(config['EXP_DIR'], f\"vae_embeddings.joblib\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM Model for Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Assign and Compile LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "LSTM Model Summary\n",
      "==================\n",
      "\n",
      "Model: \"LSTM_Autoencoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 13, 6)]           0         \n",
      "                                                                 \n",
      " encoder1 (LSTM)             (None, 13, 128)           69120     \n",
      "                                                                 \n",
      " encoder2 (LSTM)             (None, 64)                49408     \n",
      "                                                                 \n",
      " repeat_vec (RepeatVector)   (None, 13, 64)            0         \n",
      "                                                                 \n",
      " decoder1 (LSTM)             (None, 13, 64)            33024     \n",
      "                                                                 \n",
      " decoder2 (LSTM)             (None, 13, 128)           98816     \n",
      "                                                                 \n",
      " reconst (TimeDistributed)   (None, 13, 6)             774       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 251,142\n",
      "Trainable params: 251,142\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Get LSTM MODEL\n",
    "lstm_model = lstm_autoencoder(n_timesteps=config['N_WIN'] - 1,\n",
    "                              n_features=config['LATENT_DIM'])\n",
    "lstm_model.compile(loss=tf.losses.MeanSquaredError(),\n",
    "                   optimizer=tf.optimizers.Adam(\n",
    "                       learning_rate=config['LEARNING_RATE']),\n",
    "                   metrics=['mse'])\n",
    "\n",
    "# Show LSTM model summary\n",
    "print(\"\\nLSTM Model Summary\")\n",
    "print(\"==================\", end=\"\\n\\n\")\n",
    "lstm_model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "531/531 [==============================] - 35s 49ms/step - loss: 0.8665 - mse: 0.8665 - val_loss: 0.8330 - val_mse: 0.8330\n",
      "Epoch 2/1000\n",
      "531/531 [==============================] - 28s 53ms/step - loss: 0.7936 - mse: 0.7936 - val_loss: 0.7725 - val_mse: 0.7725\n",
      "Epoch 3/1000\n",
      "531/531 [==============================] - 32s 60ms/step - loss: 0.7389 - mse: 0.7389 - val_loss: 0.7235 - val_mse: 0.7235\n",
      "Epoch 4/1000\n",
      "531/531 [==============================] - 30s 56ms/step - loss: 0.6947 - mse: 0.6947 - val_loss: 0.6854 - val_mse: 0.6854\n",
      "Epoch 5/1000\n",
      "531/531 [==============================] - 34s 64ms/step - loss: 0.6641 - mse: 0.6641 - val_loss: 0.6587 - val_mse: 0.6587\n",
      "Epoch 6/1000\n",
      "531/531 [==============================] - 34s 64ms/step - loss: 0.6382 - mse: 0.6382 - val_loss: 0.6315 - val_mse: 0.6315\n",
      "Epoch 7/1000\n",
      "531/531 [==============================] - 32s 61ms/step - loss: 0.6175 - mse: 0.6175 - val_loss: 0.6135 - val_mse: 0.6135\n",
      "Epoch 8/1000\n",
      "531/531 [==============================] - 29s 55ms/step - loss: 0.6016 - mse: 0.6016 - val_loss: 0.6017 - val_mse: 0.6017\n",
      "Epoch 9/1000\n",
      "531/531 [==============================] - 30s 57ms/step - loss: 0.5877 - mse: 0.5877 - val_loss: 0.5873 - val_mse: 0.5873\n",
      "Epoch 10/1000\n",
      "531/531 [==============================] - 29s 54ms/step - loss: 0.5732 - mse: 0.5732 - val_loss: 0.5726 - val_mse: 0.5726\n",
      "Epoch 11/1000\n",
      "531/531 [==============================] - 27s 50ms/step - loss: 0.5606 - mse: 0.5606 - val_loss: 0.5607 - val_mse: 0.5607\n",
      "Epoch 12/1000\n",
      "531/531 [==============================] - 27s 50ms/step - loss: 0.5511 - mse: 0.5511 - val_loss: 0.5523 - val_mse: 0.5523\n",
      "Epoch 13/1000\n",
      "531/531 [==============================] - 27s 51ms/step - loss: 0.5435 - mse: 0.5435 - val_loss: 0.5438 - val_mse: 0.5438\n",
      "Epoch 14/1000\n",
      "531/531 [==============================] - 31s 58ms/step - loss: 0.5325 - mse: 0.5325 - val_loss: 0.5298 - val_mse: 0.5298\n",
      "Epoch 15/1000\n",
      "531/531 [==============================] - 33s 61ms/step - loss: 0.5203 - mse: 0.5203 - val_loss: 0.5196 - val_mse: 0.5196\n",
      "Epoch 16/1000\n",
      "531/531 [==============================] - 35s 66ms/step - loss: 0.5110 - mse: 0.5110 - val_loss: 0.5108 - val_mse: 0.5108\n",
      "Epoch 17/1000\n",
      "531/531 [==============================] - 31s 58ms/step - loss: 0.5047 - mse: 0.5047 - val_loss: 0.5062 - val_mse: 0.5062\n",
      "Epoch 18/1000\n",
      "531/531 [==============================] - 33s 62ms/step - loss: 0.4997 - mse: 0.4997 - val_loss: 0.5005 - val_mse: 0.5005\n",
      "Epoch 19/1000\n",
      "531/531 [==============================] - 34s 64ms/step - loss: 0.4946 - mse: 0.4946 - val_loss: 0.4947 - val_mse: 0.4947\n",
      "Epoch 20/1000\n",
      "531/531 [==============================] - 33s 62ms/step - loss: 0.4880 - mse: 0.4880 - val_loss: 0.4886 - val_mse: 0.4886\n",
      "Epoch 21/1000\n",
      "531/531 [==============================] - 27s 51ms/step - loss: 0.4810 - mse: 0.4810 - val_loss: 0.4810 - val_mse: 0.4810\n",
      "Epoch 22/1000\n",
      "531/531 [==============================] - 36s 68ms/step - loss: 0.4751 - mse: 0.4751 - val_loss: 0.4770 - val_mse: 0.4770\n",
      "Epoch 23/1000\n",
      "531/531 [==============================] - 33s 61ms/step - loss: 0.4690 - mse: 0.4690 - val_loss: 0.4706 - val_mse: 0.4706\n",
      "Epoch 24/1000\n",
      "531/531 [==============================] - 28s 53ms/step - loss: 0.4630 - mse: 0.4630 - val_loss: 0.4656 - val_mse: 0.4656\n",
      "Epoch 25/1000\n",
      "531/531 [==============================] - 29s 54ms/step - loss: 0.4584 - mse: 0.4584 - val_loss: 0.4622 - val_mse: 0.4622\n",
      "Epoch 26/1000\n",
      "531/531 [==============================] - 31s 58ms/step - loss: 0.4530 - mse: 0.4530 - val_loss: 0.4543 - val_mse: 0.4543\n",
      "Epoch 27/1000\n",
      "531/531 [==============================] - 26s 49ms/step - loss: 0.4462 - mse: 0.4462 - val_loss: 0.4478 - val_mse: 0.4478\n",
      "Epoch 28/1000\n",
      "531/531 [==============================] - 26s 49ms/step - loss: 0.4398 - mse: 0.4398 - val_loss: 0.4430 - val_mse: 0.4430\n",
      "Epoch 29/1000\n",
      "531/531 [==============================] - 31s 59ms/step - loss: 0.4343 - mse: 0.4343 - val_loss: 0.4366 - val_mse: 0.4366\n",
      "Epoch 30/1000\n",
      "531/531 [==============================] - 30s 57ms/step - loss: 0.4292 - mse: 0.4292 - val_loss: 0.4326 - val_mse: 0.4326\n",
      "Epoch 31/1000\n",
      "531/531 [==============================] - 34s 63ms/step - loss: 0.4234 - mse: 0.4234 - val_loss: 0.4245 - val_mse: 0.4245\n",
      "Epoch 32/1000\n",
      "531/531 [==============================] - 28s 53ms/step - loss: 0.4171 - mse: 0.4171 - val_loss: 0.4172 - val_mse: 0.4172\n",
      "Epoch 33/1000\n",
      "531/531 [==============================] - 27s 51ms/step - loss: 0.4109 - mse: 0.4109 - val_loss: 0.4109 - val_mse: 0.4109\n",
      "Epoch 34/1000\n",
      "531/531 [==============================] - 27s 50ms/step - loss: 0.4042 - mse: 0.4042 - val_loss: 0.4047 - val_mse: 0.4047\n",
      "Epoch 35/1000\n",
      "531/531 [==============================] - 29s 54ms/step - loss: 0.3996 - mse: 0.3996 - val_loss: 0.4005 - val_mse: 0.4005\n",
      "Epoch 36/1000\n",
      "531/531 [==============================] - 32s 60ms/step - loss: 0.3972 - mse: 0.3972 - val_loss: 0.4001 - val_mse: 0.4001\n",
      "Epoch 37/1000\n",
      "531/531 [==============================] - 29s 54ms/step - loss: 0.3951 - mse: 0.3951 - val_loss: 0.3980 - val_mse: 0.3980\n",
      "Epoch 38/1000\n",
      "531/531 [==============================] - 25s 47ms/step - loss: 0.3937 - mse: 0.3937 - val_loss: 0.3959 - val_mse: 0.3959\n",
      "Epoch 39/1000\n",
      "531/531 [==============================] - 26s 49ms/step - loss: 0.3923 - mse: 0.3923 - val_loss: 0.3950 - val_mse: 0.3950\n",
      "Epoch 40/1000\n",
      "531/531 [==============================] - 27s 51ms/step - loss: 0.3903 - mse: 0.3903 - val_loss: 0.3918 - val_mse: 0.3918\n",
      "Epoch 41/1000\n",
      "531/531 [==============================] - 28s 52ms/step - loss: 0.3870 - mse: 0.3870 - val_loss: 0.3878 - val_mse: 0.3878\n",
      "Epoch 42/1000\n",
      "531/531 [==============================] - 28s 52ms/step - loss: 0.3816 - mse: 0.3816 - val_loss: 0.3811 - val_mse: 0.3811\n",
      "Epoch 43/1000\n",
      "531/531 [==============================] - 28s 53ms/step - loss: 0.3753 - mse: 0.3753 - val_loss: 0.3770 - val_mse: 0.3770\n",
      "Epoch 44/1000\n",
      "531/531 [==============================] - 25s 48ms/step - loss: 0.3709 - mse: 0.3709 - val_loss: 0.3736 - val_mse: 0.3736\n",
      "Epoch 45/1000\n",
      "531/531 [==============================] - 26s 48ms/step - loss: 0.3682 - mse: 0.3682 - val_loss: 0.3693 - val_mse: 0.3693\n",
      "Epoch 46/1000\n",
      "531/531 [==============================] - 20s 37ms/step - loss: 0.3660 - mse: 0.3660 - val_loss: 0.3672 - val_mse: 0.3672\n",
      "Epoch 47/1000\n",
      "531/531 [==============================] - 20s 38ms/step - loss: 0.3635 - mse: 0.3635 - val_loss: 0.3646 - val_mse: 0.3646\n",
      "Epoch 48/1000\n",
      "531/531 [==============================] - 26s 49ms/step - loss: 0.3594 - mse: 0.3594 - val_loss: 0.3607 - val_mse: 0.3607\n",
      "Epoch 49/1000\n",
      "531/531 [==============================] - 25s 47ms/step - loss: 0.3555 - mse: 0.3555 - val_loss: 0.3560 - val_mse: 0.3560\n",
      "Epoch 50/1000\n",
      "531/531 [==============================] - 24s 46ms/step - loss: 0.3529 - mse: 0.3529 - val_loss: 0.3553 - val_mse: 0.3553\n",
      "Epoch 51/1000\n",
      "531/531 [==============================] - 26s 48ms/step - loss: 0.3500 - mse: 0.3500 - val_loss: 0.3525 - val_mse: 0.3525\n",
      "Epoch 52/1000\n",
      "531/531 [==============================] - 24s 44ms/step - loss: 0.3464 - mse: 0.3464 - val_loss: 0.3454 - val_mse: 0.3454\n",
      "Epoch 53/1000\n",
      "531/531 [==============================] - 23s 44ms/step - loss: 0.3423 - mse: 0.3423 - val_loss: 0.3426 - val_mse: 0.3426\n",
      "Epoch 54/1000\n",
      "531/531 [==============================] - 24s 45ms/step - loss: 0.3377 - mse: 0.3377 - val_loss: 0.3356 - val_mse: 0.3356\n",
      "Epoch 55/1000\n",
      "531/531 [==============================] - 24s 45ms/step - loss: 0.3327 - mse: 0.3327 - val_loss: 0.3319 - val_mse: 0.3319\n",
      "Epoch 56/1000\n",
      "531/531 [==============================] - 33s 63ms/step - loss: 0.3284 - mse: 0.3284 - val_loss: 0.3269 - val_mse: 0.3269\n",
      "Epoch 57/1000\n",
      "531/531 [==============================] - 30s 57ms/step - loss: 0.3238 - mse: 0.3238 - val_loss: 0.3234 - val_mse: 0.3234\n",
      "Epoch 58/1000\n",
      "531/531 [==============================] - 32s 60ms/step - loss: 0.3208 - mse: 0.3208 - val_loss: 0.3205 - val_mse: 0.3205\n",
      "Epoch 59/1000\n",
      "531/531 [==============================] - 32s 59ms/step - loss: 0.3189 - mse: 0.3189 - val_loss: 0.3191 - val_mse: 0.3191\n",
      "Epoch 60/1000\n",
      "531/531 [==============================] - 28s 53ms/step - loss: 0.3176 - mse: 0.3176 - val_loss: 0.3192 - val_mse: 0.3192\n",
      "Epoch 61/1000\n",
      "531/531 [==============================] - 32s 59ms/step - loss: 0.3165 - mse: 0.3165 - val_loss: 0.3169 - val_mse: 0.3169\n",
      "Epoch 62/1000\n",
      "531/531 [==============================] - 32s 61ms/step - loss: 0.3155 - mse: 0.3155 - val_loss: 0.3154 - val_mse: 0.3154\n",
      "Epoch 63/1000\n",
      "531/531 [==============================] - 33s 63ms/step - loss: 0.3144 - mse: 0.3144 - val_loss: 0.3140 - val_mse: 0.3140\n",
      "Epoch 64/1000\n",
      "531/531 [==============================] - 26s 50ms/step - loss: 0.3129 - mse: 0.3129 - val_loss: 0.3129 - val_mse: 0.3129\n",
      "Epoch 65/1000\n",
      "531/531 [==============================] - 29s 54ms/step - loss: 0.3101 - mse: 0.3101 - val_loss: 0.3117 - val_mse: 0.3117\n",
      "Epoch 66/1000\n",
      "531/531 [==============================] - 25s 48ms/step - loss: 0.3072 - mse: 0.3072 - val_loss: 0.3072 - val_mse: 0.3072\n",
      "Epoch 67/1000\n",
      "531/531 [==============================] - 25s 46ms/step - loss: 0.3048 - mse: 0.3048 - val_loss: 0.3043 - val_mse: 0.3043\n",
      "Epoch 68/1000\n",
      "531/531 [==============================] - 25s 47ms/step - loss: 0.3032 - mse: 0.3032 - val_loss: 0.3062 - val_mse: 0.3062\n",
      "Epoch 69/1000\n",
      "531/531 [==============================] - 31s 58ms/step - loss: 0.3020 - mse: 0.3020 - val_loss: 0.3027 - val_mse: 0.3027\n",
      "Epoch 70/1000\n",
      "531/531 [==============================] - 25s 46ms/step - loss: 0.3010 - mse: 0.3010 - val_loss: 0.3027 - val_mse: 0.3027\n",
      "Epoch 71/1000\n",
      "531/531 [==============================] - 30s 56ms/step - loss: 0.3000 - mse: 0.3000 - val_loss: 0.3026 - val_mse: 0.3026\n",
      "Epoch 72/1000\n",
      "531/531 [==============================] - 30s 57ms/step - loss: 0.2985 - mse: 0.2985 - val_loss: 0.3008 - val_mse: 0.3008\n",
      "Epoch 73/1000\n",
      "531/531 [==============================] - 30s 57ms/step - loss: 0.2972 - mse: 0.2972 - val_loss: 0.2998 - val_mse: 0.2998\n",
      "Epoch 74/1000\n",
      "531/531 [==============================] - 23s 44ms/step - loss: 0.2946 - mse: 0.2946 - val_loss: 0.2972 - val_mse: 0.2972\n",
      "Epoch 75/1000\n",
      "531/531 [==============================] - 23s 43ms/step - loss: 0.2913 - mse: 0.2913 - val_loss: 0.2945 - val_mse: 0.2945\n",
      "Epoch 76/1000\n",
      "531/531 [==============================] - 23s 43ms/step - loss: 0.2895 - mse: 0.2895 - val_loss: 0.2924 - val_mse: 0.2924\n",
      "Epoch 77/1000\n",
      "531/531 [==============================] - 23s 44ms/step - loss: 0.2878 - mse: 0.2878 - val_loss: 0.2924 - val_mse: 0.2924\n",
      "Epoch 78/1000\n",
      "531/531 [==============================] - 24s 46ms/step - loss: 0.2862 - mse: 0.2862 - val_loss: 0.2892 - val_mse: 0.2892\n",
      "Epoch 79/1000\n",
      "531/531 [==============================] - 23s 43ms/step - loss: 0.2845 - mse: 0.2845 - val_loss: 0.2862 - val_mse: 0.2862\n",
      "Epoch 80/1000\n",
      "531/531 [==============================] - 25s 46ms/step - loss: 0.2824 - mse: 0.2824 - val_loss: 0.2838 - val_mse: 0.2838\n",
      "Epoch 81/1000\n",
      "531/531 [==============================] - 25s 46ms/step - loss: 0.2803 - mse: 0.2803 - val_loss: 0.2823 - val_mse: 0.2823\n",
      "Epoch 82/1000\n",
      "531/531 [==============================] - 25s 47ms/step - loss: 0.2790 - mse: 0.2790 - val_loss: 0.2816 - val_mse: 0.2816\n",
      "Epoch 83/1000\n",
      "531/531 [==============================] - 26s 48ms/step - loss: 0.2781 - mse: 0.2781 - val_loss: 0.2793 - val_mse: 0.2793\n",
      "Epoch 84/1000\n",
      "531/531 [==============================] - 25s 47ms/step - loss: 0.2769 - mse: 0.2769 - val_loss: 0.2798 - val_mse: 0.2798\n",
      "Epoch 85/1000\n",
      "531/531 [==============================] - 27s 50ms/step - loss: 0.2758 - mse: 0.2758 - val_loss: 0.2778 - val_mse: 0.2778\n",
      "Epoch 86/1000\n",
      "531/531 [==============================] - 25s 48ms/step - loss: 0.2737 - mse: 0.2737 - val_loss: 0.2750 - val_mse: 0.2750\n",
      "Epoch 87/1000\n",
      "531/531 [==============================] - 27s 50ms/step - loss: 0.2700 - mse: 0.2700 - val_loss: 0.2723 - val_mse: 0.2723\n",
      "Epoch 88/1000\n",
      "531/531 [==============================] - 28s 53ms/step - loss: 0.2671 - mse: 0.2671 - val_loss: 0.2682 - val_mse: 0.2682\n",
      "Epoch 89/1000\n",
      "531/531 [==============================] - 36s 68ms/step - loss: 0.2651 - mse: 0.2651 - val_loss: 0.2678 - val_mse: 0.2678\n",
      "Epoch 90/1000\n",
      "531/531 [==============================] - 29s 55ms/step - loss: 0.2642 - mse: 0.2642 - val_loss: 0.2680 - val_mse: 0.2680\n",
      "Epoch 91/1000\n",
      "531/531 [==============================] - 28s 52ms/step - loss: 0.2634 - mse: 0.2634 - val_loss: 0.2671 - val_mse: 0.2671\n",
      "Epoch 92/1000\n",
      "531/531 [==============================] - 22s 42ms/step - loss: 0.2627 - mse: 0.2627 - val_loss: 0.2665 - val_mse: 0.2665\n",
      "Epoch 93/1000\n",
      "531/531 [==============================] - 29s 55ms/step - loss: 0.2622 - mse: 0.2622 - val_loss: 0.2643 - val_mse: 0.2643\n",
      "Epoch 94/1000\n",
      "531/531 [==============================] - 29s 55ms/step - loss: 0.2615 - mse: 0.2615 - val_loss: 0.2655 - val_mse: 0.2655\n",
      "Epoch 95/1000\n",
      "531/531 [==============================] - 28s 53ms/step - loss: 0.2608 - mse: 0.2608 - val_loss: 0.2649 - val_mse: 0.2649\n",
      "Epoch 96/1000\n",
      "531/531 [==============================] - 29s 55ms/step - loss: 0.2601 - mse: 0.2601 - val_loss: 0.2631 - val_mse: 0.2631\n",
      "Epoch 97/1000\n",
      "531/531 [==============================] - 30s 56ms/step - loss: 0.2590 - mse: 0.2590 - val_loss: 0.2629 - val_mse: 0.2629\n",
      "Epoch 98/1000\n",
      "531/531 [==============================] - 30s 57ms/step - loss: 0.2573 - mse: 0.2573 - val_loss: 0.2613 - val_mse: 0.2613\n",
      "Epoch 99/1000\n",
      "531/531 [==============================] - 32s 59ms/step - loss: 0.2555 - mse: 0.2555 - val_loss: 0.2585 - val_mse: 0.2585\n",
      "Epoch 100/1000\n",
      "531/531 [==============================] - 30s 56ms/step - loss: 0.2535 - mse: 0.2535 - val_loss: 0.2556 - val_mse: 0.2556\n",
      "Epoch 101/1000\n",
      "531/531 [==============================] - 29s 55ms/step - loss: 0.2516 - mse: 0.2516 - val_loss: 0.2543 - val_mse: 0.2543\n",
      "Epoch 102/1000\n",
      "531/531 [==============================] - 28s 53ms/step - loss: 0.2501 - mse: 0.2501 - val_loss: 0.2518 - val_mse: 0.2518\n",
      "Epoch 103/1000\n",
      "531/531 [==============================] - 28s 53ms/step - loss: 0.2482 - mse: 0.2482 - val_loss: 0.2523 - val_mse: 0.2523\n",
      "Epoch 104/1000\n",
      "531/531 [==============================] - 28s 52ms/step - loss: 0.2448 - mse: 0.2448 - val_loss: 0.2480 - val_mse: 0.2480\n",
      "Epoch 105/1000\n",
      "531/531 [==============================] - 27s 51ms/step - loss: 0.2412 - mse: 0.2412 - val_loss: 0.2436 - val_mse: 0.2436\n",
      "Epoch 106/1000\n",
      "531/531 [==============================] - 26s 49ms/step - loss: 0.2383 - mse: 0.2383 - val_loss: 0.2427 - val_mse: 0.2427\n",
      "Epoch 107/1000\n",
      "531/531 [==============================] - 26s 49ms/step - loss: 0.2352 - mse: 0.2352 - val_loss: 0.2383 - val_mse: 0.2383\n",
      "Epoch 108/1000\n",
      "531/531 [==============================] - 27s 51ms/step - loss: 0.2323 - mse: 0.2323 - val_loss: 0.2344 - val_mse: 0.2344\n",
      "Epoch 109/1000\n",
      "531/531 [==============================] - 27s 50ms/step - loss: 0.2304 - mse: 0.2304 - val_loss: 0.2335 - val_mse: 0.2335\n",
      "Epoch 110/1000\n",
      "531/531 [==============================] - 25s 48ms/step - loss: 0.2286 - mse: 0.2286 - val_loss: 0.2332 - val_mse: 0.2332\n",
      "Epoch 111/1000\n",
      "531/531 [==============================] - 26s 49ms/step - loss: 0.2277 - mse: 0.2277 - val_loss: 0.2302 - val_mse: 0.2302\n",
      "Epoch 112/1000\n",
      "531/531 [==============================] - 25s 47ms/step - loss: 0.2269 - mse: 0.2269 - val_loss: 0.2316 - val_mse: 0.2316\n",
      "Epoch 113/1000\n",
      "531/531 [==============================] - 25s 47ms/step - loss: 0.2264 - mse: 0.2264 - val_loss: 0.2293 - val_mse: 0.2293\n",
      "Epoch 114/1000\n",
      "531/531 [==============================] - 24s 46ms/step - loss: 0.2258 - mse: 0.2258 - val_loss: 0.2300 - val_mse: 0.2300\n",
      "Epoch 115/1000\n",
      "531/531 [==============================] - 24s 46ms/step - loss: 0.2252 - mse: 0.2252 - val_loss: 0.2318 - val_mse: 0.2318\n",
      "Epoch 116/1000\n",
      "531/531 [==============================] - 24s 45ms/step - loss: 0.2247 - mse: 0.2247 - val_loss: 0.2293 - val_mse: 0.2293\n",
      "Epoch 117/1000\n",
      "531/531 [==============================] - 24s 45ms/step - loss: 0.2242 - mse: 0.2242 - val_loss: 0.2292 - val_mse: 0.2292\n",
      "Epoch 118/1000\n",
      "531/531 [==============================] - 24s 45ms/step - loss: 0.2237 - mse: 0.2237 - val_loss: 0.2285 - val_mse: 0.2285\n",
      "Epoch 119/1000\n",
      "531/531 [==============================] - 25s 47ms/step - loss: 0.2231 - mse: 0.2231 - val_loss: 0.2288 - val_mse: 0.2288\n",
      "Epoch 120/1000\n",
      "531/531 [==============================] - 23s 43ms/step - loss: 0.2223 - mse: 0.2223 - val_loss: 0.2277 - val_mse: 0.2277\n",
      "Epoch 121/1000\n",
      "531/531 [==============================] - 24s 45ms/step - loss: 0.2218 - mse: 0.2218 - val_loss: 0.2276 - val_mse: 0.2276\n",
      "Epoch 122/1000\n",
      "531/531 [==============================] - 24s 44ms/step - loss: 0.2212 - mse: 0.2212 - val_loss: 0.2273 - val_mse: 0.2273\n",
      "Epoch 123/1000\n",
      "531/531 [==============================] - 30s 56ms/step - loss: 0.2206 - mse: 0.2206 - val_loss: 0.2277 - val_mse: 0.2277\n",
      "Epoch 124/1000\n",
      "531/531 [==============================] - 25s 47ms/step - loss: 0.2200 - mse: 0.2200 - val_loss: 0.2261 - val_mse: 0.2261\n",
      "Epoch 125/1000\n",
      "531/531 [==============================] - 26s 48ms/step - loss: 0.2196 - mse: 0.2196 - val_loss: 0.2267 - val_mse: 0.2267\n",
      "Epoch 126/1000\n",
      "531/531 [==============================] - 24s 44ms/step - loss: 0.2191 - mse: 0.2191 - val_loss: 0.2268 - val_mse: 0.2268\n",
      "Epoch 127/1000\n",
      "531/531 [==============================] - 24s 45ms/step - loss: 0.2184 - mse: 0.2184 - val_loss: 0.2246 - val_mse: 0.2246\n",
      "Epoch 128/1000\n",
      "531/531 [==============================] - 24s 45ms/step - loss: 0.2177 - mse: 0.2177 - val_loss: 0.2236 - val_mse: 0.2236\n",
      "Epoch 129/1000\n",
      "531/531 [==============================] - 24s 46ms/step - loss: 0.2165 - mse: 0.2165 - val_loss: 0.2212 - val_mse: 0.2212\n",
      "Epoch 130/1000\n",
      "531/531 [==============================] - 23s 44ms/step - loss: 0.2150 - mse: 0.2150 - val_loss: 0.2202 - val_mse: 0.2202\n",
      "Epoch 131/1000\n",
      "531/531 [==============================] - 24s 45ms/step - loss: 0.2138 - mse: 0.2138 - val_loss: 0.2188 - val_mse: 0.2188\n",
      "Epoch 132/1000\n",
      "531/531 [==============================] - 23s 44ms/step - loss: 0.2130 - mse: 0.2130 - val_loss: 0.2185 - val_mse: 0.2185\n",
      "Epoch 133/1000\n",
      "531/531 [==============================] - 23s 44ms/step - loss: 0.2124 - mse: 0.2124 - val_loss: 0.2166 - val_mse: 0.2166\n",
      "Epoch 134/1000\n",
      "531/531 [==============================] - 24s 45ms/step - loss: 0.2118 - mse: 0.2118 - val_loss: 0.2172 - val_mse: 0.2172\n",
      "Epoch 135/1000\n",
      "531/531 [==============================] - 24s 44ms/step - loss: 0.2114 - mse: 0.2114 - val_loss: 0.2172 - val_mse: 0.2172\n",
      "Epoch 136/1000\n",
      "531/531 [==============================] - 23s 44ms/step - loss: 0.2104 - mse: 0.2104 - val_loss: 0.2170 - val_mse: 0.2170\n",
      "Epoch 137/1000\n",
      "531/531 [==============================] - 23s 43ms/step - loss: 0.2097 - mse: 0.2097 - val_loss: 0.2167 - val_mse: 0.2167\n",
      "Epoch 138/1000\n",
      "531/531 [==============================] - 23s 43ms/step - loss: 0.2079 - mse: 0.2079 - val_loss: 0.2121 - val_mse: 0.2121\n",
      "Epoch 139/1000\n",
      "531/531 [==============================] - 23s 44ms/step - loss: 0.2049 - mse: 0.2049 - val_loss: 0.2103 - val_mse: 0.2103\n",
      "Epoch 140/1000\n",
      "531/531 [==============================] - 24s 45ms/step - loss: 0.2022 - mse: 0.2022 - val_loss: 0.2092 - val_mse: 0.2092\n",
      "Epoch 141/1000\n",
      "531/531 [==============================] - 24s 44ms/step - loss: 0.2005 - mse: 0.2005 - val_loss: 0.2051 - val_mse: 0.2051\n",
      "Epoch 142/1000\n",
      "531/531 [==============================] - 25s 46ms/step - loss: 0.1987 - mse: 0.1987 - val_loss: 0.2036 - val_mse: 0.2036\n",
      "Epoch 143/1000\n",
      "531/531 [==============================] - 23s 44ms/step - loss: 0.1969 - mse: 0.1969 - val_loss: 0.2026 - val_mse: 0.2026\n",
      "Epoch 144/1000\n",
      "531/531 [==============================] - 23s 44ms/step - loss: 0.1944 - mse: 0.1944 - val_loss: 0.2014 - val_mse: 0.2014\n",
      "Epoch 145/1000\n",
      "531/531 [==============================] - 23s 44ms/step - loss: 0.1918 - mse: 0.1918 - val_loss: 0.1956 - val_mse: 0.1956\n",
      "Epoch 146/1000\n",
      "531/531 [==============================] - 23s 43ms/step - loss: 0.1896 - mse: 0.1896 - val_loss: 0.1946 - val_mse: 0.1946\n",
      "Epoch 147/1000\n",
      "531/531 [==============================] - 23s 43ms/step - loss: 0.1884 - mse: 0.1884 - val_loss: 0.1952 - val_mse: 0.1952\n",
      "Epoch 148/1000\n",
      "531/531 [==============================] - 23s 43ms/step - loss: 0.1873 - mse: 0.1873 - val_loss: 0.1925 - val_mse: 0.1925\n",
      "Epoch 149/1000\n",
      "531/531 [==============================] - 24s 44ms/step - loss: 0.1863 - mse: 0.1863 - val_loss: 0.1918 - val_mse: 0.1918\n",
      "Epoch 150/1000\n",
      "531/531 [==============================] - 27s 52ms/step - loss: 0.1853 - mse: 0.1853 - val_loss: 0.1913 - val_mse: 0.1913\n",
      "Epoch 151/1000\n",
      "531/531 [==============================] - 25s 47ms/step - loss: 0.1844 - mse: 0.1844 - val_loss: 0.1911 - val_mse: 0.1911\n",
      "Epoch 152/1000\n",
      "531/531 [==============================] - 28s 53ms/step - loss: 0.1836 - mse: 0.1836 - val_loss: 0.1907 - val_mse: 0.1907\n",
      "Epoch 153/1000\n",
      "531/531 [==============================] - 25s 48ms/step - loss: 0.1830 - mse: 0.1830 - val_loss: 0.1896 - val_mse: 0.1896\n",
      "Epoch 154/1000\n",
      "531/531 [==============================] - 28s 52ms/step - loss: 0.1823 - mse: 0.1823 - val_loss: 0.1897 - val_mse: 0.1897\n",
      "Epoch 155/1000\n",
      "531/531 [==============================] - 27s 51ms/step - loss: 0.1821 - mse: 0.1821 - val_loss: 0.1896 - val_mse: 0.1896\n",
      "Epoch 156/1000\n",
      "531/531 [==============================] - 30s 57ms/step - loss: 0.1817 - mse: 0.1817 - val_loss: 0.1891 - val_mse: 0.1891\n",
      "Epoch 157/1000\n",
      "531/531 [==============================] - 24s 45ms/step - loss: 0.1814 - mse: 0.1814 - val_loss: 0.1886 - val_mse: 0.1886\n",
      "Epoch 158/1000\n",
      "531/531 [==============================] - 22s 41ms/step - loss: 0.1808 - mse: 0.1808 - val_loss: 0.1884 - val_mse: 0.1884\n",
      "Epoch 159/1000\n",
      "531/531 [==============================] - 23s 43ms/step - loss: 0.1806 - mse: 0.1806 - val_loss: 0.1879 - val_mse: 0.1879\n",
      "Epoch 160/1000\n",
      "531/531 [==============================] - 23s 44ms/step - loss: 0.1804 - mse: 0.1804 - val_loss: 0.1877 - val_mse: 0.1877\n",
      "Epoch 161/1000\n",
      "531/531 [==============================] - 32s 60ms/step - loss: 0.1799 - mse: 0.1799 - val_loss: 0.1867 - val_mse: 0.1867\n",
      "Epoch 162/1000\n",
      "531/531 [==============================] - 27s 51ms/step - loss: 0.1794 - mse: 0.1794 - val_loss: 0.1857 - val_mse: 0.1857\n",
      "Epoch 163/1000\n",
      "531/531 [==============================] - 28s 53ms/step - loss: 0.1787 - mse: 0.1787 - val_loss: 0.1861 - val_mse: 0.1861\n",
      "Epoch 164/1000\n",
      "531/531 [==============================] - 24s 45ms/step - loss: 0.1776 - mse: 0.1776 - val_loss: 0.1831 - val_mse: 0.1831\n",
      "Epoch 165/1000\n",
      "531/531 [==============================] - 26s 49ms/step - loss: 0.1759 - mse: 0.1759 - val_loss: 0.1814 - val_mse: 0.1814\n",
      "Epoch 166/1000\n",
      "531/531 [==============================] - 25s 47ms/step - loss: 0.1750 - mse: 0.1750 - val_loss: 0.1795 - val_mse: 0.1795\n",
      "Epoch 167/1000\n",
      "531/531 [==============================] - 24s 45ms/step - loss: 0.1742 - mse: 0.1742 - val_loss: 0.1786 - val_mse: 0.1786\n",
      "Epoch 168/1000\n",
      "531/531 [==============================] - 24s 45ms/step - loss: 0.1738 - mse: 0.1738 - val_loss: 0.1787 - val_mse: 0.1787\n",
      "Epoch 169/1000\n",
      "531/531 [==============================] - 28s 53ms/step - loss: 0.1733 - mse: 0.1733 - val_loss: 0.1793 - val_mse: 0.1793\n",
      "Epoch 170/1000\n",
      "531/531 [==============================] - 25s 47ms/step - loss: 0.1731 - mse: 0.1731 - val_loss: 0.1778 - val_mse: 0.1778\n",
      "Epoch 171/1000\n",
      "531/531 [==============================] - 22s 41ms/step - loss: 0.1727 - mse: 0.1727 - val_loss: 0.1784 - val_mse: 0.1784\n",
      "Epoch 172/1000\n",
      "531/531 [==============================] - 21s 40ms/step - loss: 0.1723 - mse: 0.1723 - val_loss: 0.1785 - val_mse: 0.1785\n",
      "Epoch 173/1000\n",
      "531/531 [==============================] - 22s 41ms/step - loss: 0.1719 - mse: 0.1719 - val_loss: 0.1786 - val_mse: 0.1786\n",
      "Epoch 174/1000\n",
      "531/531 [==============================] - 21s 40ms/step - loss: 0.1717 - mse: 0.1717 - val_loss: 0.1777 - val_mse: 0.1777\n",
      "Epoch 175/1000\n",
      "531/531 [==============================] - 21s 40ms/step - loss: 0.1713 - mse: 0.1713 - val_loss: 0.1776 - val_mse: 0.1776\n",
      "Epoch 176/1000\n",
      "531/531 [==============================] - 22s 41ms/step - loss: 0.1708 - mse: 0.1708 - val_loss: 0.1798 - val_mse: 0.1798\n",
      "Epoch 177/1000\n",
      "531/531 [==============================] - 22s 41ms/step - loss: 0.1705 - mse: 0.1705 - val_loss: 0.1785 - val_mse: 0.1785\n",
      "Epoch 178/1000\n",
      "531/531 [==============================] - 21s 40ms/step - loss: 0.1700 - mse: 0.1700 - val_loss: 0.1775 - val_mse: 0.1775\n",
      "Epoch 179/1000\n",
      "531/531 [==============================] - 22s 42ms/step - loss: 0.1697 - mse: 0.1697 - val_loss: 0.1772 - val_mse: 0.1772\n",
      "Epoch 180/1000\n",
      "531/531 [==============================] - 22s 41ms/step - loss: 0.1694 - mse: 0.1694 - val_loss: 0.1777 - val_mse: 0.1777\n",
      "Epoch 181/1000\n",
      "531/531 [==============================] - 21s 39ms/step - loss: 0.1690 - mse: 0.1690 - val_loss: 0.1777 - val_mse: 0.1777\n",
      "Epoch 182/1000\n",
      "531/531 [==============================] - 16s 31ms/step - loss: 0.1687 - mse: 0.1687 - val_loss: 0.1782 - val_mse: 0.1782\n",
      "Epoch 183/1000\n",
      "531/531 [==============================] - 24s 45ms/step - loss: 0.1686 - mse: 0.1686 - val_loss: 0.1778 - val_mse: 0.1778\n",
      "Epoch 184/1000\n",
      "531/531 [==============================] - 24s 45ms/step - loss: 0.1683 - mse: 0.1683 - val_loss: 0.1758 - val_mse: 0.1758\n",
      "Epoch 185/1000\n",
      "531/531 [==============================] - 23s 44ms/step - loss: 0.1680 - mse: 0.1680 - val_loss: 0.1763 - val_mse: 0.1763\n",
      "Epoch 186/1000\n",
      "531/531 [==============================] - 23s 44ms/step - loss: 0.1679 - mse: 0.1679 - val_loss: 0.1764 - val_mse: 0.1764\n",
      "Epoch 187/1000\n",
      "531/531 [==============================] - 26s 50ms/step - loss: 0.1676 - mse: 0.1676 - val_loss: 0.1773 - val_mse: 0.1773\n",
      "Epoch 188/1000\n",
      "531/531 [==============================] - 31s 59ms/step - loss: 0.1674 - mse: 0.1674 - val_loss: 0.1775 - val_mse: 0.1775\n",
      "Epoch 189/1000\n",
      "531/531 [==============================] - 32s 61ms/step - loss: 0.1673 - mse: 0.1673 - val_loss: 0.1758 - val_mse: 0.1758\n",
      "Epoch 190/1000\n",
      "531/531 [==============================] - 33s 62ms/step - loss: 0.1669 - mse: 0.1669 - val_loss: 0.1768 - val_mse: 0.1768\n",
      "Epoch 191/1000\n",
      "531/531 [==============================] - 32s 60ms/step - loss: 0.1668 - mse: 0.1668 - val_loss: 0.1750 - val_mse: 0.1750\n",
      "Epoch 192/1000\n",
      "531/531 [==============================] - 44s 82ms/step - loss: 0.1665 - mse: 0.1665 - val_loss: 0.1745 - val_mse: 0.1745\n",
      "Epoch 193/1000\n",
      "531/531 [==============================] - 40s 75ms/step - loss: 0.1661 - mse: 0.1661 - val_loss: 0.1749 - val_mse: 0.1749\n",
      "Epoch 194/1000\n",
      "531/531 [==============================] - 33s 63ms/step - loss: 0.1655 - mse: 0.1655 - val_loss: 0.1728 - val_mse: 0.1728\n",
      "Epoch 195/1000\n",
      "531/531 [==============================] - 47s 89ms/step - loss: 0.1644 - mse: 0.1644 - val_loss: 0.1722 - val_mse: 0.1722\n",
      "Epoch 196/1000\n",
      "531/531 [==============================] - 32s 59ms/step - loss: 0.1623 - mse: 0.1623 - val_loss: 0.1674 - val_mse: 0.1674\n",
      "Epoch 197/1000\n",
      "531/531 [==============================] - 43s 81ms/step - loss: 0.1591 - mse: 0.1591 - val_loss: 0.1646 - val_mse: 0.1646\n",
      "Epoch 198/1000\n",
      "531/531 [==============================] - 40s 75ms/step - loss: 0.1551 - mse: 0.1551 - val_loss: 0.1612 - val_mse: 0.1612\n",
      "Epoch 199/1000\n",
      "531/531 [==============================] - 34s 64ms/step - loss: 0.1508 - mse: 0.1508 - val_loss: 0.1587 - val_mse: 0.1587\n",
      "Epoch 200/1000\n",
      "531/531 [==============================] - 28s 52ms/step - loss: 0.1479 - mse: 0.1479 - val_loss: 0.1534 - val_mse: 0.1534\n",
      "Epoch 201/1000\n",
      "531/531 [==============================] - 35s 66ms/step - loss: 0.1466 - mse: 0.1466 - val_loss: 0.1547 - val_mse: 0.1547\n",
      "Epoch 202/1000\n",
      "531/531 [==============================] - 37s 70ms/step - loss: 0.1455 - mse: 0.1455 - val_loss: 0.1535 - val_mse: 0.1535\n",
      "Epoch 203/1000\n",
      "531/531 [==============================] - 31s 59ms/step - loss: 0.1449 - mse: 0.1449 - val_loss: 0.1522 - val_mse: 0.1522\n",
      "Epoch 204/1000\n",
      "531/531 [==============================] - 25s 47ms/step - loss: 0.1446 - mse: 0.1446 - val_loss: 0.1521 - val_mse: 0.1521\n",
      "Epoch 205/1000\n",
      "531/531 [==============================] - 26s 49ms/step - loss: 0.1441 - mse: 0.1441 - val_loss: 0.1517 - val_mse: 0.1517\n",
      "Epoch 206/1000\n",
      "531/531 [==============================] - 35s 67ms/step - loss: 0.1439 - mse: 0.1439 - val_loss: 0.1514 - val_mse: 0.1514\n",
      "Epoch 207/1000\n",
      "531/531 [==============================] - 36s 67ms/step - loss: 0.1435 - mse: 0.1435 - val_loss: 0.1516 - val_mse: 0.1516\n",
      "Epoch 208/1000\n",
      "531/531 [==============================] - 25s 47ms/step - loss: 0.1433 - mse: 0.1433 - val_loss: 0.1505 - val_mse: 0.1505\n",
      "Epoch 209/1000\n",
      "531/531 [==============================] - 30s 56ms/step - loss: 0.1433 - mse: 0.1433 - val_loss: 0.1514 - val_mse: 0.1514\n",
      "Epoch 210/1000\n",
      "531/531 [==============================] - 39s 74ms/step - loss: 0.1429 - mse: 0.1429 - val_loss: 0.1520 - val_mse: 0.1520\n",
      "Epoch 211/1000\n",
      "531/531 [==============================] - 27s 51ms/step - loss: 0.1429 - mse: 0.1429 - val_loss: 0.1510 - val_mse: 0.1510\n",
      "Epoch 212/1000\n",
      "531/531 [==============================] - 25s 47ms/step - loss: 0.1427 - mse: 0.1427 - val_loss: 0.1506 - val_mse: 0.1506\n",
      "Epoch 213/1000\n",
      "531/531 [==============================] - 27s 51ms/step - loss: 0.1425 - mse: 0.1425 - val_loss: 0.1505 - val_mse: 0.1505\n",
      "Epoch 214/1000\n",
      "531/531 [==============================] - 28s 52ms/step - loss: 0.1425 - mse: 0.1425 - val_loss: 0.1501 - val_mse: 0.1501\n",
      "Epoch 215/1000\n",
      "531/531 [==============================] - 31s 58ms/step - loss: 0.1422 - mse: 0.1422 - val_loss: 0.1505 - val_mse: 0.1505\n",
      "Epoch 216/1000\n",
      "531/531 [==============================] - 29s 55ms/step - loss: 0.1422 - mse: 0.1422 - val_loss: 0.1505 - val_mse: 0.1505\n",
      "Epoch 217/1000\n",
      "531/531 [==============================] - 26s 48ms/step - loss: 0.1420 - mse: 0.1420 - val_loss: 0.1498 - val_mse: 0.1498\n",
      "Epoch 218/1000\n",
      "531/531 [==============================] - 25s 48ms/step - loss: 0.1418 - mse: 0.1418 - val_loss: 0.1505 - val_mse: 0.1505\n",
      "Epoch 219/1000\n",
      "531/531 [==============================] - 33s 61ms/step - loss: 0.1416 - mse: 0.1416 - val_loss: 0.1504 - val_mse: 0.1504\n",
      "Epoch 220/1000\n",
      "531/531 [==============================] - 24s 46ms/step - loss: 0.1415 - mse: 0.1415 - val_loss: 0.1487 - val_mse: 0.1487\n",
      "Epoch 221/1000\n",
      "531/531 [==============================] - 28s 53ms/step - loss: 0.1409 - mse: 0.1409 - val_loss: 0.1490 - val_mse: 0.1490\n",
      "Epoch 222/1000\n",
      "531/531 [==============================] - 25s 47ms/step - loss: 0.1399 - mse: 0.1399 - val_loss: 0.1457 - val_mse: 0.1457\n",
      "Epoch 223/1000\n",
      "531/531 [==============================] - 37s 70ms/step - loss: 0.1386 - mse: 0.1386 - val_loss: 0.1443 - val_mse: 0.1443\n",
      "Epoch 224/1000\n",
      "531/531 [==============================] - 47s 89ms/step - loss: 0.1374 - mse: 0.1374 - val_loss: 0.1425 - val_mse: 0.1425\n",
      "Epoch 225/1000\n",
      "531/531 [==============================] - 46s 88ms/step - loss: 0.1366 - mse: 0.1366 - val_loss: 0.1419 - val_mse: 0.1419\n",
      "Epoch 226/1000\n",
      "531/531 [==============================] - 42s 78ms/step - loss: 0.1362 - mse: 0.1362 - val_loss: 0.1415 - val_mse: 0.1415\n",
      "Epoch 227/1000\n",
      "531/531 [==============================] - 43s 81ms/step - loss: 0.1359 - mse: 0.1359 - val_loss: 0.1410 - val_mse: 0.1410\n",
      "Epoch 228/1000\n",
      "531/531 [==============================] - 35s 66ms/step - loss: 0.1355 - mse: 0.1355 - val_loss: 0.1414 - val_mse: 0.1414\n",
      "Epoch 229/1000\n",
      "531/531 [==============================] - 26s 49ms/step - loss: 0.1353 - mse: 0.1353 - val_loss: 0.1412 - val_mse: 0.1412\n",
      "Epoch 230/1000\n",
      "531/531 [==============================] - 26s 49ms/step - loss: 0.1350 - mse: 0.1350 - val_loss: 0.1416 - val_mse: 0.1416\n",
      "Epoch 231/1000\n",
      "531/531 [==============================] - 33s 62ms/step - loss: 0.1347 - mse: 0.1347 - val_loss: 0.1417 - val_mse: 0.1417\n",
      "Epoch 232/1000\n",
      "531/531 [==============================] - 48s 91ms/step - loss: 0.1343 - mse: 0.1343 - val_loss: 0.1405 - val_mse: 0.1405\n",
      "Epoch 233/1000\n",
      "531/531 [==============================] - 44s 83ms/step - loss: 0.1338 - mse: 0.1338 - val_loss: 0.1406 - val_mse: 0.1406\n",
      "Epoch 234/1000\n",
      "531/531 [==============================] - 40s 76ms/step - loss: 0.1334 - mse: 0.1334 - val_loss: 0.1416 - val_mse: 0.1416\n",
      "Epoch 235/1000\n",
      "531/531 [==============================] - 29s 54ms/step - loss: 0.1326 - mse: 0.1326 - val_loss: 0.1399 - val_mse: 0.1399\n",
      "Epoch 236/1000\n",
      "531/531 [==============================] - 24s 45ms/step - loss: 0.1324 - mse: 0.1324 - val_loss: 0.1397 - val_mse: 0.1397\n",
      "Epoch 237/1000\n",
      "530/531 [============================>.] - ETA: 0s - loss: 0.1319 - mse: 0.1319"
     ]
    }
   ],
   "source": [
    "# Assign checkpoint paths\n",
    "lstm_ckpt_path = join(\n",
    "    config['EXP_DIR'], \"lstm_checkpoint\", \"ckpt\")\n",
    "    \n",
    "# Callbacks for LSTM\n",
    "early_stopping_callback = EarlyStopping(monitor='val_loss',\n",
    "                                        patience=config['PATIENCE'],\n",
    "                                        mode='min',\n",
    "                                        restore_best_weights=True)\n",
    "checkpoint_callback = ModelCheckpoint(lstm_ckpt_path,\n",
    "                                      monitor='val_loss',\n",
    "                                      verbose=0,\n",
    "                                      mode='min',\n",
    "                                      save_best_only=True,\n",
    "                                      save_weights_only=True)\n",
    "\n",
    "if not os.path.isfile(lstm_ckpt_path + \".index\"):\n",
    "    # Train LSTM\n",
    "    lstm_history = lstm_model.fit(embed_gen.x_train, embed_gen.y_train,\n",
    "                                  validation_split=0.05,\n",
    "                                  batch_size=config['BATCH_SIZE'],\n",
    "                                  epochs=config['EPOCH'],\n",
    "                                  callbacks=[\n",
    "                                      early_stopping_callback, checkpoint_callback],\n",
    "                                  verbose=1)\n",
    "\n",
    "    # Export model history\n",
    "    export_history(lstm_history, join(\n",
    "        config['EXP_DIR'], data.id + \"_lstm_history.csv\"))\n",
    "\n",
    "    # Plot loss curve\n",
    "    # print(\"\\nLSTM Loss Curve\")\n",
    "    # print(\"===============\", end=\"\\n\")\n",
    "    plot.loss_curve(config, lstm_history, ref=data.id + \"_LSTM\", save_plot=True,\n",
    "                    close_plot=True)\n",
    "else:\n",
    "    lstm_model.load_weights(lstm_ckpt_path)\n",
    "    print(\"LSTM model weights loaded from:\")\n",
    "    print(lstm_ckpt_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('venv_dl')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f5f53b166ca748d5dc505def4d94272f31d2e9774072ce34fa297f04ce988b3f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
