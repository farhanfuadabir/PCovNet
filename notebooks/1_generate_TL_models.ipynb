{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: e:\\Repositories\\PCovNet\\src\n",
      "Found GPU at: /device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "# Default packages\n",
    "import os\n",
    "from os.path import join\n",
    "from datetime import date, datetime, timedelta\n",
    "from joblib import load, dump\n",
    "\n",
    "# Set current directory to \"src\"\n",
    "os.chdir(join(os.getcwd(), os.pardir, \"src\"))\n",
    "print(f\"Current working directory: {os.getcwd()}\")\n",
    "\n",
    "# Installed packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.backend import clear_session\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "# Defined modules\n",
    "from util.util import *\n",
    "# from config.config import *\n",
    "from process.dataloader import DataTL\n",
    "from process.embed_gen import EmbedGenTL\n",
    "from process.evaluate import *\n",
    "from models import lstm_vae, cnn_vae\n",
    "from models.lstm_ae import lstm_autoencoder\n",
    "from models.lstm import basic_lstm\n",
    "from visualize import plot\n",
    "\n",
    "# Set TF log level to minimum\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"\n",
    "\n",
    "# Check GPU availability\n",
    "device_name = tf.test.gpu_device_name()\n",
    "if device_name != '/device:GPU:0':\n",
    "    raise SystemError('GPU device not found')\n",
    "print('Found GPU at: {}'.format(device_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    'EXP_NAME': \"healthy_models\",\n",
    "    'EXP_PHASE': \"phase1\",\n",
    "    'EXP_GROUP': \"healthy\",\n",
    "    'AUGMENT': True,\n",
    "    'LEN_WIN': 24,\n",
    "    'N_WIN': 7,\n",
    "    'LATENT_DIM': 6,\n",
    "    'BATCH_SIZE': 64,\n",
    "    'VAL_SPLIT': 0.05,\n",
    "    'LEARNING_RATE': 0.0002,\n",
    "    'EPOCH': 1000,\n",
    "    'PATIENCE': 10,\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "config\n",
      "{\n",
      "    \"EXP_NAME\": \"healthy_models\",\n",
      "    \"EXP_PHASE\": \"phase1\",\n",
      "    \"EXP_GROUP\": \"healthy\",\n",
      "    \"AUGMENT\": true,\n",
      "    \"LEN_WIN\": 24,\n",
      "    \"N_WIN\": 7,\n",
      "    \"LATENT_DIM\": 6,\n",
      "    \"BATCH_SIZE\": 64,\n",
      "    \"VAL_SPLIT\": 0.05,\n",
      "    \"LEARNING_RATE\": 0.0002,\n",
      "    \"EPOCH\": 1000,\n",
      "    \"PATIENCE\": 10,\n",
      "    \"EXP_DIR\": \"e:\\\\Repositories\\\\PCovNet\\\\src\\\\..\\\\experiment\\\\healthy_models_24_7\",\n",
      "    \"DATA_DIR\": \"e:\\\\Repositories\\\\PCovNet\\\\src\\\\..\\\\data\\\\raw\\\\phase1\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Dictionaries of dataset paths\n",
    "DATA_DIR_DICT = {\n",
    "    'phase1': join(os.getcwd(), os.pardir, \"data\", \"raw\", \"phase1\"),\n",
    "    'phase2': join(os.getcwd(), os.pardir, \"data\", \"raw\", \"phase2\")\n",
    "}\n",
    "INFO_DIR_DICT = {\n",
    "    'covid_phase1': join(os.getcwd(), os.pardir, \"data\", \"external\", \"covid_phase1_info.csv\"),\n",
    "    'covid_phase2': join(os.getcwd(), os.pardir, \"data\", \"external\", \"covid_phase2_info.csv\"),\n",
    "    'healthy_phase1': join(os.getcwd(), os.pardir, \"data\", \"external\", \"healthy_phase1_info.csv\"),\n",
    "    'non-covid_phase1': join(os.getcwd(), os.pardir, \"data\", \"external\", \"non-covid_phase1_info.csv\"),\n",
    "}\n",
    "\n",
    "# Import subject info\n",
    "subject_info = pd.read_csv(\n",
    "    INFO_DIR_DICT[f\"{config['EXP_GROUP']}_{config['EXP_PHASE']}\"])\n",
    "\n",
    "\n",
    "# Assign experiment directory\n",
    "config['EXP_DIR'] = join(os.getcwd(), os.pardir, \"experiment\",\n",
    "                         f\"{config['EXP_NAME']}_{config['LEN_WIN']}_{config['N_WIN']}\")\n",
    "# timestamp = datetime.now().strftime(f\"%Y-%m-%d %H-%M__\")\n",
    "# config['EXP_DIR'] = join(os.getcwd(), os.pardir, \"experiment\",\n",
    "#                          timestamp + config['EXP_NAME'])\n",
    "handle_dir(config['EXP_DIR'])\n",
    "\n",
    "\n",
    "# Add DATA_DIR\n",
    "config['DATA_DIR'] = DATA_DIR_DICT[config['EXP_PHASE']]\n",
    "\n",
    "\n",
    "# Export config\n",
    "export_json(config, join(config['EXP_DIR'], \"config.json\"),\n",
    "            print_json=True)\n",
    "\n",
    "# Start logging\n",
    "with open(join(config['EXP_DIR'], \"log.txt\"), 'w', encoding='utf-8') as f:\n",
    "    for key, value in config.items():\n",
    "        f.write(f\"{key}: {value}\\n\")\n",
    "    f.write(\"\\n\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import & Process Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 63/63 [02:26<00:00,  2.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            Subject Info\n",
      "            ============\n",
      "            Phase:                phase1\n",
      "            Group:                healthy\n",
      "            \n",
      "            Dataset Shape\n",
      "            =============\n",
      "            VAE Train:            (55208, 24, 1)\n",
      "            VAE Train Aug:        (441664, 24, 1)\n",
      "            LSTM Train:           (46136, 7, 24, 1)\n",
      "        \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "if 'Symptom Onset' in subject_info.columns:\n",
    "    subject_info.drop(['Symptom Onset'], axis=1)\n",
    "\n",
    "data_obj_path = join(config['EXP_DIR'],\n",
    "                     f\"{config['LEN_WIN']}-{config['N_WIN']}_data.joblib\")\n",
    "\n",
    "if not os.path.isfile(data_obj_path):\n",
    "    # Prepare data\n",
    "    data = DataTL(config, subject_info)\n",
    "\n",
    "    # Print data info\n",
    "    data.print_info()\n",
    "\n",
    "    # Export data object\n",
    "    dump(data, data_obj_path)\n",
    "else:\n",
    "    data = load(data_obj_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VAE Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Assign and Compile VAE Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "VAE Model Summary\n",
      "=================\n",
      "\n",
      "Model: \"Encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " encoder_input (InputLayer)     [(None, 24, 1)]      0           []                               \n",
      "                                                                                                  \n",
      " encoder1 (Conv1D)              (None, 12, 128)      512         ['encoder_input[0][0]']          \n",
      "                                                                                                  \n",
      " encoder2 (Conv1D)              (None, 6, 64)        24640       ['encoder1[0][0]']               \n",
      "                                                                                                  \n",
      " encoder3 (Conv1D)              (None, 3, 32)        6176        ['encoder2[0][0]']               \n",
      "                                                                                                  \n",
      " flatten (Flatten)              (None, 96)           0           ['encoder3[0][0]']               \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 6)            582         ['flatten[0][0]']                \n",
      "                                                                                                  \n",
      " z_mean (Dense)                 (None, 6)            42          ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      " z_var (Dense)                  (None, 6)            42          ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      " z (Sampling)                   (None, 6)            0           ['z_mean[0][0]',                 \n",
      "                                                                  'z_var[0][0]']                  \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 31,994\n",
      "Trainable params: 31,994\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "\n",
      "\n",
      "Model: \"Decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 6)]               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 96)                672       \n",
      "                                                                 \n",
      " reshape (Reshape)           (None, 3, 32)             0         \n",
      "                                                                 \n",
      " decoder1 (Conv1DTranspose)  (None, 6, 32)             3104      \n",
      "                                                                 \n",
      " decoder2 (Conv1DTranspose)  (None, 12, 64)            6208      \n",
      "                                                                 \n",
      " decoder3 (Conv1DTranspose)  (None, 24, 128)           24704     \n",
      "                                                                 \n",
      " conv1d_transpose (Conv1DTra  (None, 24, 1)            385       \n",
      " nspose)                                                         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 35,073\n",
      "Trainable params: 35,073\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "\n",
      "Model: \"Variational_AutoEncoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " Input (InputLayer)          [(None, 24, 1)]           0         \n",
      "                                                                 \n",
      " Encoder (Functional)        [(None, 6),               31994     \n",
      "                              (None, 6),                         \n",
      "                              (None, 6)]                         \n",
      "                                                                 \n",
      " Decoder (Functional)        (None, 24, 1)             35073     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 67,067\n",
      "Trainable params: 67,067\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Get VAE model\n",
    "vae_model = cnn_vae.VAE(n_timesteps=config['LEN_WIN'],\n",
    "                        n_channels=data.train_dataset_vae.shape[-1],\n",
    "                        latent_dim=config['LATENT_DIM'])\n",
    "vae_model.compile(loss=tf.losses.MeanSquaredError(),\n",
    "                  optimizer=tf.optimizers.Adam(\n",
    "                      learning_rate=config['LEARNING_RATE']),\n",
    "                  metrics=[tf.metrics.MeanSquaredError()])\n",
    "\n",
    "# Show VAE model summary\n",
    "print(\"\\nVAE Model Summary\")\n",
    "print(\"=================\", end=\"\\n\\n\")\n",
    "vae_model.print_summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train VAE Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "820/820 [==============================] - 33s 35ms/step - loss: 22.4488 - reconst_loss: 21.4981 - kl_loss: 0.9507 - val_loss: 19.6534 - val_reconst_loss: 18.2204 - val_kl_loss: 1.4330\n",
      "Epoch 2/1000\n",
      "820/820 [==============================] - 31s 37ms/step - loss: 17.2752 - reconst_loss: 15.8690 - kl_loss: 1.4062 - val_loss: 16.4845 - val_reconst_loss: 14.7483 - val_kl_loss: 1.7362\n",
      "Epoch 3/1000\n",
      "820/820 [==============================] - 31s 37ms/step - loss: 16.0470 - reconst_loss: 14.6927 - kl_loss: 1.3543 - val_loss: 16.1678 - val_reconst_loss: 15.1613 - val_kl_loss: 1.0065\n",
      "Epoch 4/1000\n",
      "820/820 [==============================] - 34s 41ms/step - loss: 15.8534 - reconst_loss: 14.5849 - kl_loss: 1.2685 - val_loss: 16.0448 - val_reconst_loss: 15.0208 - val_kl_loss: 1.0240\n",
      "Epoch 5/1000\n",
      "820/820 [==============================] - 32s 38ms/step - loss: 15.6479 - reconst_loss: 14.4752 - kl_loss: 1.1727 - val_loss: 15.6315 - val_reconst_loss: 14.5701 - val_kl_loss: 1.0614\n",
      "Epoch 6/1000\n",
      "820/820 [==============================] - 27s 33ms/step - loss: 15.4840 - reconst_loss: 14.3867 - kl_loss: 1.0973 - val_loss: 15.5877 - val_reconst_loss: 14.5155 - val_kl_loss: 1.0722\n",
      "Epoch 7/1000\n",
      "820/820 [==============================] - 27s 33ms/step - loss: 15.3534 - reconst_loss: 14.3024 - kl_loss: 1.0510 - val_loss: 15.4074 - val_reconst_loss: 14.2759 - val_kl_loss: 1.1314\n",
      "Epoch 8/1000\n",
      "820/820 [==============================] - 26s 31ms/step - loss: 15.2641 - reconst_loss: 14.2059 - kl_loss: 1.0582 - val_loss: 15.3900 - val_reconst_loss: 14.3544 - val_kl_loss: 1.0356\n",
      "Epoch 9/1000\n",
      "820/820 [==============================] - 29s 35ms/step - loss: 15.2126 - reconst_loss: 14.1416 - kl_loss: 1.0710 - val_loss: 15.5438 - val_reconst_loss: 14.5273 - val_kl_loss: 1.0165\n",
      "Epoch 10/1000\n",
      "820/820 [==============================] - 27s 33ms/step - loss: 15.2054 - reconst_loss: 14.1086 - kl_loss: 1.0968 - val_loss: 15.3377 - val_reconst_loss: 14.2155 - val_kl_loss: 1.1222\n",
      "Epoch 11/1000\n",
      "820/820 [==============================] - 26s 32ms/step - loss: 15.1529 - reconst_loss: 14.0544 - kl_loss: 1.0985 - val_loss: 15.2262 - val_reconst_loss: 14.1357 - val_kl_loss: 1.0905\n",
      "Epoch 12/1000\n",
      "820/820 [==============================] - 26s 32ms/step - loss: 15.1411 - reconst_loss: 14.0198 - kl_loss: 1.1213 - val_loss: 15.4442 - val_reconst_loss: 14.1160 - val_kl_loss: 1.3282\n",
      "Epoch 13/1000\n",
      "820/820 [==============================] - 26s 32ms/step - loss: 15.1294 - reconst_loss: 13.9818 - kl_loss: 1.1477 - val_loss: 15.2650 - val_reconst_loss: 14.1374 - val_kl_loss: 1.1276\n",
      "Epoch 14/1000\n",
      "820/820 [==============================] - 26s 32ms/step - loss: 15.1315 - reconst_loss: 13.9737 - kl_loss: 1.1577 - val_loss: 15.2443 - val_reconst_loss: 14.0601 - val_kl_loss: 1.1842\n",
      "Epoch 15/1000\n",
      "820/820 [==============================] - 28s 34ms/step - loss: 15.1056 - reconst_loss: 13.9357 - kl_loss: 1.1699 - val_loss: 15.2274 - val_reconst_loss: 14.0193 - val_kl_loss: 1.2082\n",
      "Epoch 16/1000\n",
      "820/820 [==============================] - 27s 33ms/step - loss: 15.0984 - reconst_loss: 13.9123 - kl_loss: 1.1861 - val_loss: 15.2326 - val_reconst_loss: 14.0583 - val_kl_loss: 1.1743\n",
      "Epoch 17/1000\n",
      "820/820 [==============================] - 26s 32ms/step - loss: 15.0817 - reconst_loss: 13.9004 - kl_loss: 1.1812 - val_loss: 15.2271 - val_reconst_loss: 14.0372 - val_kl_loss: 1.1898\n",
      "Epoch 18/1000\n",
      "820/820 [==============================] - 26s 32ms/step - loss: 15.0858 - reconst_loss: 13.8858 - kl_loss: 1.2000 - val_loss: 15.2155 - val_reconst_loss: 14.0646 - val_kl_loss: 1.1509\n",
      "Epoch 19/1000\n",
      "820/820 [==============================] - 28s 34ms/step - loss: 15.0600 - reconst_loss: 13.8695 - kl_loss: 1.1904 - val_loss: 15.2240 - val_reconst_loss: 13.9212 - val_kl_loss: 1.3028\n",
      "Epoch 20/1000\n",
      "820/820 [==============================] - 26s 32ms/step - loss: 15.0619 - reconst_loss: 13.8681 - kl_loss: 1.1938 - val_loss: 15.3027 - val_reconst_loss: 14.0836 - val_kl_loss: 1.2190\n",
      "Epoch 21/1000\n",
      "820/820 [==============================] - 27s 33ms/step - loss: 15.0487 - reconst_loss: 13.8549 - kl_loss: 1.1938 - val_loss: 15.1945 - val_reconst_loss: 14.0364 - val_kl_loss: 1.1581\n",
      "Epoch 22/1000\n",
      "820/820 [==============================] - 27s 33ms/step - loss: 15.0488 - reconst_loss: 13.8505 - kl_loss: 1.1984 - val_loss: 15.2993 - val_reconst_loss: 14.0290 - val_kl_loss: 1.2703\n",
      "Epoch 23/1000\n",
      "820/820 [==============================] - 27s 32ms/step - loss: 15.0571 - reconst_loss: 13.8562 - kl_loss: 1.2010 - val_loss: 15.1573 - val_reconst_loss: 13.9453 - val_kl_loss: 1.2120\n",
      "Epoch 24/1000\n",
      "820/820 [==============================] - 27s 33ms/step - loss: 15.0347 - reconst_loss: 13.8304 - kl_loss: 1.2042 - val_loss: 15.2069 - val_reconst_loss: 14.0190 - val_kl_loss: 1.1879\n",
      "Epoch 25/1000\n",
      "820/820 [==============================] - 28s 35ms/step - loss: 15.0374 - reconst_loss: 13.8332 - kl_loss: 1.2042 - val_loss: 15.1579 - val_reconst_loss: 13.9806 - val_kl_loss: 1.1773\n",
      "Epoch 26/1000\n",
      "820/820 [==============================] - 32s 38ms/step - loss: 15.0251 - reconst_loss: 13.8230 - kl_loss: 1.2021 - val_loss: 15.1114 - val_reconst_loss: 13.8504 - val_kl_loss: 1.2610\n",
      "Epoch 27/1000\n",
      "820/820 [==============================] - 30s 36ms/step - loss: 15.0264 - reconst_loss: 13.8227 - kl_loss: 1.2037 - val_loss: 15.1116 - val_reconst_loss: 13.8634 - val_kl_loss: 1.2482\n",
      "Epoch 28/1000\n",
      "820/820 [==============================] - 26s 32ms/step - loss: 15.0256 - reconst_loss: 13.8176 - kl_loss: 1.2080 - val_loss: 15.2201 - val_reconst_loss: 14.0600 - val_kl_loss: 1.1601\n",
      "Epoch 29/1000\n",
      "820/820 [==============================] - 28s 34ms/step - loss: 15.0261 - reconst_loss: 13.8225 - kl_loss: 1.2036 - val_loss: 15.1434 - val_reconst_loss: 13.9358 - val_kl_loss: 1.2076\n",
      "Epoch 30/1000\n",
      "820/820 [==============================] - 29s 35ms/step - loss: 15.0213 - reconst_loss: 13.8091 - kl_loss: 1.2122 - val_loss: 15.1491 - val_reconst_loss: 13.9850 - val_kl_loss: 1.1641\n",
      "Epoch 31/1000\n",
      "820/820 [==============================] - 35s 43ms/step - loss: 15.0244 - reconst_loss: 13.8108 - kl_loss: 1.2137 - val_loss: 15.1569 - val_reconst_loss: 13.8537 - val_kl_loss: 1.3032\n",
      "Epoch 32/1000\n",
      "820/820 [==============================] - 29s 35ms/step - loss: 15.0095 - reconst_loss: 13.8005 - kl_loss: 1.2090 - val_loss: 15.0926 - val_reconst_loss: 13.9401 - val_kl_loss: 1.1524\n",
      "Epoch 33/1000\n",
      "820/820 [==============================] - 31s 37ms/step - loss: 15.0127 - reconst_loss: 13.7988 - kl_loss: 1.2140 - val_loss: 15.1667 - val_reconst_loss: 13.8748 - val_kl_loss: 1.2919\n",
      "Epoch 34/1000\n",
      "820/820 [==============================] - 27s 33ms/step - loss: 15.0112 - reconst_loss: 13.8011 - kl_loss: 1.2102 - val_loss: 15.1454 - val_reconst_loss: 13.9521 - val_kl_loss: 1.1933\n",
      "Epoch 35/1000\n",
      "820/820 [==============================] - 29s 36ms/step - loss: 15.0238 - reconst_loss: 13.7926 - kl_loss: 1.2312 - val_loss: 15.1822 - val_reconst_loss: 13.8592 - val_kl_loss: 1.3230\n",
      "Epoch 36/1000\n",
      "820/820 [==============================] - 32s 39ms/step - loss: 15.0063 - reconst_loss: 13.7974 - kl_loss: 1.2088 - val_loss: 15.1087 - val_reconst_loss: 13.8672 - val_kl_loss: 1.2415\n",
      "Epoch 37/1000\n",
      "820/820 [==============================] - 33s 40ms/step - loss: 15.0038 - reconst_loss: 13.7965 - kl_loss: 1.2073 - val_loss: 15.0797 - val_reconst_loss: 13.7705 - val_kl_loss: 1.3092\n",
      "Epoch 38/1000\n",
      "820/820 [==============================] - 28s 34ms/step - loss: 15.0076 - reconst_loss: 13.7934 - kl_loss: 1.2141 - val_loss: 15.0977 - val_reconst_loss: 13.8785 - val_kl_loss: 1.2191\n",
      "Epoch 39/1000\n",
      "820/820 [==============================] - 28s 35ms/step - loss: 15.0106 - reconst_loss: 13.7959 - kl_loss: 1.2146 - val_loss: 15.1036 - val_reconst_loss: 13.7949 - val_kl_loss: 1.3087\n",
      "Epoch 40/1000\n",
      "820/820 [==============================] - 27s 33ms/step - loss: 15.0011 - reconst_loss: 13.7893 - kl_loss: 1.2118 - val_loss: 15.1451 - val_reconst_loss: 13.8725 - val_kl_loss: 1.2725\n",
      "Epoch 41/1000\n",
      "820/820 [==============================] - 31s 37ms/step - loss: 15.0067 - reconst_loss: 13.7921 - kl_loss: 1.2146 - val_loss: 15.1153 - val_reconst_loss: 13.9365 - val_kl_loss: 1.1788\n",
      "Epoch 42/1000\n",
      "820/820 [==============================] - 39s 48ms/step - loss: 14.9977 - reconst_loss: 13.7886 - kl_loss: 1.2092 - val_loss: 15.0612 - val_reconst_loss: 13.7555 - val_kl_loss: 1.3057\n",
      "Epoch 43/1000\n",
      "820/820 [==============================] - 28s 34ms/step - loss: 15.0081 - reconst_loss: 13.7878 - kl_loss: 1.2202 - val_loss: 15.1200 - val_reconst_loss: 13.7910 - val_kl_loss: 1.3290\n",
      "Epoch 44/1000\n",
      "820/820 [==============================] - 27s 33ms/step - loss: 14.9972 - reconst_loss: 13.7865 - kl_loss: 1.2107 - val_loss: 15.0533 - val_reconst_loss: 13.8084 - val_kl_loss: 1.2449\n",
      "Epoch 45/1000\n",
      "820/820 [==============================] - 30s 37ms/step - loss: 14.9984 - reconst_loss: 13.7838 - kl_loss: 1.2146 - val_loss: 15.1447 - val_reconst_loss: 13.8036 - val_kl_loss: 1.3411\n",
      "Epoch 46/1000\n",
      "820/820 [==============================] - 40s 49ms/step - loss: 14.9941 - reconst_loss: 13.7796 - kl_loss: 1.2145 - val_loss: 15.0789 - val_reconst_loss: 13.7879 - val_kl_loss: 1.2909\n",
      "Epoch 47/1000\n",
      "820/820 [==============================] - 36s 43ms/step - loss: 14.9845 - reconst_loss: 13.7808 - kl_loss: 1.2037 - val_loss: 15.0788 - val_reconst_loss: 13.8410 - val_kl_loss: 1.2378\n",
      "Epoch 48/1000\n",
      "820/820 [==============================] - 35s 43ms/step - loss: 14.9956 - reconst_loss: 13.7846 - kl_loss: 1.2110 - val_loss: 15.1429 - val_reconst_loss: 13.8927 - val_kl_loss: 1.2502\n",
      "Epoch 49/1000\n",
      "820/820 [==============================] - 38s 47ms/step - loss: 14.9949 - reconst_loss: 13.7762 - kl_loss: 1.2188 - val_loss: 15.1243 - val_reconst_loss: 13.8643 - val_kl_loss: 1.2600\n",
      "Epoch 50/1000\n",
      "820/820 [==============================] - 33s 40ms/step - loss: 14.9955 - reconst_loss: 13.7767 - kl_loss: 1.2188 - val_loss: 15.1700 - val_reconst_loss: 13.9450 - val_kl_loss: 1.2250\n",
      "Epoch 51/1000\n",
      "820/820 [==============================] - 37s 46ms/step - loss: 14.9888 - reconst_loss: 13.7747 - kl_loss: 1.2141 - val_loss: 15.0979 - val_reconst_loss: 13.8354 - val_kl_loss: 1.2625\n",
      "Epoch 52/1000\n",
      "820/820 [==============================] - 31s 38ms/step - loss: 14.9983 - reconst_loss: 13.7800 - kl_loss: 1.2184 - val_loss: 15.1223 - val_reconst_loss: 13.8525 - val_kl_loss: 1.2698\n",
      "Epoch 53/1000\n",
      "820/820 [==============================] - 31s 38ms/step - loss: 14.9970 - reconst_loss: 13.7824 - kl_loss: 1.2147 - val_loss: 15.0297 - val_reconst_loss: 13.7276 - val_kl_loss: 1.3021\n",
      "Epoch 54/1000\n",
      "820/820 [==============================] - 30s 37ms/step - loss: 14.9912 - reconst_loss: 13.7736 - kl_loss: 1.2176 - val_loss: 15.1294 - val_reconst_loss: 13.8638 - val_kl_loss: 1.2655\n",
      "Epoch 55/1000\n",
      "820/820 [==============================] - 27s 32ms/step - loss: 14.9846 - reconst_loss: 13.7739 - kl_loss: 1.2107 - val_loss: 15.0682 - val_reconst_loss: 13.7912 - val_kl_loss: 1.2770\n",
      "Epoch 56/1000\n",
      "820/820 [==============================] - 26s 32ms/step - loss: 14.9945 - reconst_loss: 13.7801 - kl_loss: 1.2144 - val_loss: 15.0541 - val_reconst_loss: 13.7965 - val_kl_loss: 1.2575\n",
      "Epoch 57/1000\n",
      "820/820 [==============================] - 29s 35ms/step - loss: 14.9965 - reconst_loss: 13.7850 - kl_loss: 1.2115 - val_loss: 15.0887 - val_reconst_loss: 13.7853 - val_kl_loss: 1.3033\n",
      "Epoch 58/1000\n",
      "820/820 [==============================] - 35s 42ms/step - loss: 14.9885 - reconst_loss: 13.7782 - kl_loss: 1.2102 - val_loss: 15.0739 - val_reconst_loss: 13.8219 - val_kl_loss: 1.2520\n",
      "Epoch 59/1000\n",
      "820/820 [==============================] - 35s 43ms/step - loss: 14.9850 - reconst_loss: 13.7763 - kl_loss: 1.2087 - val_loss: 15.0676 - val_reconst_loss: 13.7483 - val_kl_loss: 1.3194\n",
      "Epoch 60/1000\n",
      "820/820 [==============================] - 30s 37ms/step - loss: 14.9717 - reconst_loss: 13.7769 - kl_loss: 1.1947 - val_loss: 15.0571 - val_reconst_loss: 13.8328 - val_kl_loss: 1.2243\n",
      "Epoch 61/1000\n",
      "820/820 [==============================] - 40s 49ms/step - loss: 14.9935 - reconst_loss: 13.7924 - kl_loss: 1.2011 - val_loss: 15.0591 - val_reconst_loss: 13.8204 - val_kl_loss: 1.2387\n",
      "Epoch 62/1000\n",
      "820/820 [==============================] - 52s 64ms/step - loss: 14.9837 - reconst_loss: 13.7822 - kl_loss: 1.2015 - val_loss: 15.0882 - val_reconst_loss: 13.7512 - val_kl_loss: 1.3370\n",
      "Epoch 63/1000\n",
      "820/820 [==============================] - 57s 69ms/step - loss: 14.9941 - reconst_loss: 13.7730 - kl_loss: 1.2211 - val_loss: 15.0678 - val_reconst_loss: 13.8462 - val_kl_loss: 1.2216\n"
     ]
    }
   ],
   "source": [
    "# Assign checkpoint paths\n",
    "vae_ckpt_path = join(\n",
    "    config['EXP_DIR'], \"vae_checkpoint\", \"ckpt\")\n",
    "\n",
    "# Callbacks for VAE\n",
    "early_stopping_callback = EarlyStopping(monitor='val_loss',\n",
    "                                        patience=config['PATIENCE'],\n",
    "                                        mode='min',\n",
    "                                        restore_best_weights=True)\n",
    "checkpoint_callback = ModelCheckpoint(vae_ckpt_path,\n",
    "                                      monitor='val_loss',\n",
    "                                      verbose=0,\n",
    "                                      mode='min',\n",
    "                                      save_best_only=True,\n",
    "                                      save_weights_only=True)\n",
    "\n",
    "if not os.path.isfile(vae_ckpt_path + \".index\"):\n",
    "    # Train VAE\n",
    "    vae_history = vae_model.fit(data.train_dataset_vae,\n",
    "                                validation_split=config['VAL_SPLIT'],\n",
    "                                batch_size=config['BATCH_SIZE'],\n",
    "                                epochs=config['EPOCH'],\n",
    "                                shuffle=False,\n",
    "                                verbose=1,\n",
    "                                callbacks=[early_stopping_callback, checkpoint_callback])\n",
    "\n",
    "    # Export model history\n",
    "    export_history(vae_history, join(\n",
    "        config['EXP_DIR'], \"vae_history.csv\"))\n",
    "\n",
    "    # Plot loss curve\n",
    "    # print(\"\\nVAE Loss Curve\")\n",
    "    # print(\"==============\", end=\"\\n\")\n",
    "    plot.loss_curve(config, vae_history, ref=\"_VAE\", save_plot=True,\n",
    "                    close_plot=True)\n",
    "\n",
    "else:\n",
    "    vae_model.load_weights(vae_ckpt_path)\n",
    "    print(\"VAE model weights loaded from:\")\n",
    "    print(vae_ckpt_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating embeddings for train dataset... \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 46136/46136 [12:52<00:00, 59.73it/s] \n"
     ]
    }
   ],
   "source": [
    "if not os.path.isfile(join(config['EXP_DIR'], f\"vae_embeddings.joblib\")):\n",
    "    # Get embedding dataset\n",
    "    embed_gen = EmbedGenTL(config, vae_model,\n",
    "                           data, verbose=True)\n",
    "\n",
    "    # Save embed_gen object\n",
    "    dump(embed_gen, join(config['EXP_DIR'],\n",
    "                         f\"vae_embeddings.joblib\"))\n",
    "else:\n",
    "    # Load embed_gen object\n",
    "    embed_gen = load(join(config['EXP_DIR'], f\"vae_embeddings.joblib\"))\n",
    "    print(\"Embeddings loaded from:\")\n",
    "    print(join(config['EXP_DIR'], f\"vae_embeddings.joblib\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM Model for Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Assign and Compile LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "LSTM Model Summary\n",
      "==================\n",
      "\n",
      "Model: \"LSTM_Autoencoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 6, 6)]            0         \n",
      "                                                                 \n",
      " encoder1 (LSTM)             (None, 6, 128)            69120     \n",
      "                                                                 \n",
      " encoder2 (LSTM)             (None, 64)                49408     \n",
      "                                                                 \n",
      " repeat_vec (RepeatVector)   (None, 6, 64)             0         \n",
      "                                                                 \n",
      " decoder1 (LSTM)             (None, 6, 64)             33024     \n",
      "                                                                 \n",
      " decoder2 (LSTM)             (None, 6, 128)            98816     \n",
      "                                                                 \n",
      " reconst (TimeDistributed)   (None, 6, 6)              774       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 251,142\n",
      "Trainable params: 251,142\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Get LSTM MODEL\n",
    "lstm_model = lstm_autoencoder(n_timesteps=config['N_WIN'] - 1,\n",
    "                              n_features=config['LATENT_DIM'])\n",
    "lstm_model.compile(loss=tf.losses.MeanSquaredError(),\n",
    "                   optimizer=tf.optimizers.Adam(\n",
    "                       learning_rate=config['LEARNING_RATE']),\n",
    "                   metrics=['mse'])\n",
    "\n",
    "# Show LSTM model summary\n",
    "print(\"\\nLSTM Model Summary\")\n",
    "print(\"==================\", end=\"\\n\\n\")\n",
    "lstm_model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "685/685 [==============================] - 32s 38ms/step - loss: 0.8345 - mse: 0.8345 - val_loss: 0.7451 - val_mse: 0.7451\n",
      "Epoch 2/1000\n",
      "685/685 [==============================] - 26s 38ms/step - loss: 0.6761 - mse: 0.6761 - val_loss: 0.5964 - val_mse: 0.5964\n",
      "Epoch 3/1000\n",
      "685/685 [==============================] - 25s 36ms/step - loss: 0.5485 - mse: 0.5485 - val_loss: 0.5023 - val_mse: 0.5023\n",
      "Epoch 4/1000\n",
      "685/685 [==============================] - 25s 37ms/step - loss: 0.4674 - mse: 0.4674 - val_loss: 0.4493 - val_mse: 0.4493\n",
      "Epoch 5/1000\n",
      "685/685 [==============================] - 27s 39ms/step - loss: 0.4242 - mse: 0.4242 - val_loss: 0.4047 - val_mse: 0.4047\n",
      "Epoch 6/1000\n",
      "685/685 [==============================] - 26s 38ms/step - loss: 0.3712 - mse: 0.3712 - val_loss: 0.3496 - val_mse: 0.3496\n",
      "Epoch 7/1000\n",
      "685/685 [==============================] - 27s 39ms/step - loss: 0.3273 - mse: 0.3273 - val_loss: 0.3202 - val_mse: 0.3202\n",
      "Epoch 8/1000\n",
      "685/685 [==============================] - 27s 40ms/step - loss: 0.3041 - mse: 0.3041 - val_loss: 0.2944 - val_mse: 0.2944\n",
      "Epoch 9/1000\n",
      "685/685 [==============================] - 28s 40ms/step - loss: 0.2765 - mse: 0.2765 - val_loss: 0.2693 - val_mse: 0.2693\n",
      "Epoch 10/1000\n",
      "685/685 [==============================] - 28s 40ms/step - loss: 0.2534 - mse: 0.2534 - val_loss: 0.2419 - val_mse: 0.2419\n",
      "Epoch 11/1000\n",
      "685/685 [==============================] - 28s 41ms/step - loss: 0.2159 - mse: 0.2159 - val_loss: 0.1995 - val_mse: 0.1995\n",
      "Epoch 12/1000\n",
      "685/685 [==============================] - 29s 42ms/step - loss: 0.1820 - mse: 0.1820 - val_loss: 0.1787 - val_mse: 0.1787\n",
      "Epoch 13/1000\n",
      "685/685 [==============================] - 28s 42ms/step - loss: 0.1703 - mse: 0.1703 - val_loss: 0.1704 - val_mse: 0.1704\n",
      "Epoch 14/1000\n",
      "685/685 [==============================] - 28s 41ms/step - loss: 0.1662 - mse: 0.1662 - val_loss: 0.1684 - val_mse: 0.1684\n",
      "Epoch 15/1000\n",
      "685/685 [==============================] - 28s 42ms/step - loss: 0.1642 - mse: 0.1642 - val_loss: 0.1665 - val_mse: 0.1665\n",
      "Epoch 16/1000\n",
      "685/685 [==============================] - 28s 41ms/step - loss: 0.1629 - mse: 0.1629 - val_loss: 0.1657 - val_mse: 0.1657\n",
      "Epoch 17/1000\n",
      "685/685 [==============================] - 29s 42ms/step - loss: 0.1619 - mse: 0.1619 - val_loss: 0.1638 - val_mse: 0.1638\n",
      "Epoch 18/1000\n",
      "685/685 [==============================] - 29s 42ms/step - loss: 0.1605 - mse: 0.1605 - val_loss: 0.1622 - val_mse: 0.1622\n",
      "Epoch 19/1000\n",
      "685/685 [==============================] - 29s 42ms/step - loss: 0.1584 - mse: 0.1584 - val_loss: 0.1591 - val_mse: 0.1591\n",
      "Epoch 20/1000\n",
      "685/685 [==============================] - 29s 42ms/step - loss: 0.1569 - mse: 0.1569 - val_loss: 0.1585 - val_mse: 0.1585\n",
      "Epoch 21/1000\n",
      "685/685 [==============================] - 29s 43ms/step - loss: 0.1561 - mse: 0.1561 - val_loss: 0.1587 - val_mse: 0.1587\n",
      "Epoch 22/1000\n",
      "685/685 [==============================] - 29s 42ms/step - loss: 0.1556 - mse: 0.1556 - val_loss: 0.1578 - val_mse: 0.1578\n",
      "Epoch 23/1000\n",
      "685/685 [==============================] - 29s 43ms/step - loss: 0.1552 - mse: 0.1552 - val_loss: 0.1570 - val_mse: 0.1570\n",
      "Epoch 24/1000\n",
      "685/685 [==============================] - 30s 44ms/step - loss: 0.1551 - mse: 0.1551 - val_loss: 0.1580 - val_mse: 0.1580\n",
      "Epoch 25/1000\n",
      "685/685 [==============================] - 31s 45ms/step - loss: 0.1548 - mse: 0.1548 - val_loss: 0.1575 - val_mse: 0.1575\n",
      "Epoch 26/1000\n",
      "685/685 [==============================] - 30s 44ms/step - loss: 0.1547 - mse: 0.1547 - val_loss: 0.1569 - val_mse: 0.1569\n",
      "Epoch 27/1000\n",
      "685/685 [==============================] - 29s 42ms/step - loss: 0.1545 - mse: 0.1545 - val_loss: 0.1566 - val_mse: 0.1566\n",
      "Epoch 28/1000\n",
      "685/685 [==============================] - 31s 45ms/step - loss: 0.1544 - mse: 0.1544 - val_loss: 0.1571 - val_mse: 0.1571\n",
      "Epoch 29/1000\n",
      "685/685 [==============================] - 31s 45ms/step - loss: 0.1544 - mse: 0.1544 - val_loss: 0.1572 - val_mse: 0.1572\n",
      "Epoch 30/1000\n",
      "685/685 [==============================] - 31s 45ms/step - loss: 0.1542 - mse: 0.1542 - val_loss: 0.1570 - val_mse: 0.1570\n",
      "Epoch 31/1000\n",
      "685/685 [==============================] - 40s 58ms/step - loss: 0.1541 - mse: 0.1541 - val_loss: 0.1561 - val_mse: 0.1561\n",
      "Epoch 32/1000\n",
      "685/685 [==============================] - 31s 46ms/step - loss: 0.1540 - mse: 0.1540 - val_loss: 0.1557 - val_mse: 0.1557\n",
      "Epoch 33/1000\n",
      "685/685 [==============================] - 31s 46ms/step - loss: 0.1540 - mse: 0.1540 - val_loss: 0.1563 - val_mse: 0.1563\n",
      "Epoch 34/1000\n",
      "685/685 [==============================] - 30s 45ms/step - loss: 0.1538 - mse: 0.1538 - val_loss: 0.1568 - val_mse: 0.1568\n",
      "Epoch 35/1000\n",
      "685/685 [==============================] - 31s 45ms/step - loss: 0.1538 - mse: 0.1538 - val_loss: 0.1559 - val_mse: 0.1559\n",
      "Epoch 36/1000\n",
      "685/685 [==============================] - 31s 45ms/step - loss: 0.1537 - mse: 0.1537 - val_loss: 0.1561 - val_mse: 0.1561\n",
      "Epoch 37/1000\n",
      "685/685 [==============================] - 32s 47ms/step - loss: 0.1536 - mse: 0.1536 - val_loss: 0.1560 - val_mse: 0.1560\n",
      "Epoch 38/1000\n",
      "685/685 [==============================] - 31s 45ms/step - loss: 0.1537 - mse: 0.1537 - val_loss: 0.1556 - val_mse: 0.1556\n",
      "Epoch 39/1000\n",
      "685/685 [==============================] - 31s 46ms/step - loss: 0.1535 - mse: 0.1535 - val_loss: 0.1558 - val_mse: 0.1558\n",
      "Epoch 40/1000\n",
      "685/685 [==============================] - 31s 45ms/step - loss: 0.1535 - mse: 0.1535 - val_loss: 0.1560 - val_mse: 0.1560\n",
      "Epoch 41/1000\n",
      "685/685 [==============================] - 33s 48ms/step - loss: 0.1534 - mse: 0.1534 - val_loss: 0.1563 - val_mse: 0.1563\n",
      "Epoch 42/1000\n",
      "685/685 [==============================] - 32s 46ms/step - loss: 0.1534 - mse: 0.1534 - val_loss: 0.1555 - val_mse: 0.1555\n",
      "Epoch 43/1000\n",
      "685/685 [==============================] - 32s 47ms/step - loss: 0.1534 - mse: 0.1534 - val_loss: 0.1553 - val_mse: 0.1553\n",
      "Epoch 44/1000\n",
      "685/685 [==============================] - 31s 46ms/step - loss: 0.1533 - mse: 0.1533 - val_loss: 0.1562 - val_mse: 0.1562\n",
      "Epoch 45/1000\n",
      "685/685 [==============================] - 32s 47ms/step - loss: 0.1534 - mse: 0.1534 - val_loss: 0.1554 - val_mse: 0.1554\n",
      "Epoch 46/1000\n",
      "685/685 [==============================] - 31s 45ms/step - loss: 0.1532 - mse: 0.1532 - val_loss: 0.1554 - val_mse: 0.1554\n",
      "Epoch 47/1000\n",
      "685/685 [==============================] - 31s 45ms/step - loss: 0.1532 - mse: 0.1532 - val_loss: 0.1556 - val_mse: 0.1556\n",
      "Epoch 48/1000\n",
      "685/685 [==============================] - 32s 46ms/step - loss: 0.1532 - mse: 0.1532 - val_loss: 0.1552 - val_mse: 0.1552\n",
      "Epoch 49/1000\n",
      "685/685 [==============================] - 33s 48ms/step - loss: 0.1532 - mse: 0.1532 - val_loss: 0.1557 - val_mse: 0.1557\n",
      "Epoch 50/1000\n",
      "685/685 [==============================] - 32s 47ms/step - loss: 0.1531 - mse: 0.1531 - val_loss: 0.1552 - val_mse: 0.1552\n",
      "Epoch 51/1000\n",
      "685/685 [==============================] - 32s 47ms/step - loss: 0.1532 - mse: 0.1532 - val_loss: 0.1551 - val_mse: 0.1551\n",
      "Epoch 52/1000\n",
      "685/685 [==============================] - 32s 46ms/step - loss: 0.1530 - mse: 0.1530 - val_loss: 0.1555 - val_mse: 0.1555\n",
      "Epoch 53/1000\n",
      "685/685 [==============================] - 32s 47ms/step - loss: 0.1531 - mse: 0.1531 - val_loss: 0.1556 - val_mse: 0.1556\n",
      "Epoch 54/1000\n",
      "685/685 [==============================] - 33s 48ms/step - loss: 0.1530 - mse: 0.1530 - val_loss: 0.1557 - val_mse: 0.1557\n",
      "Epoch 55/1000\n",
      "685/685 [==============================] - 32s 46ms/step - loss: 0.1530 - mse: 0.1530 - val_loss: 0.1549 - val_mse: 0.1549\n",
      "Epoch 56/1000\n",
      "685/685 [==============================] - 31s 46ms/step - loss: 0.1530 - mse: 0.1530 - val_loss: 0.1552 - val_mse: 0.1552\n",
      "Epoch 57/1000\n",
      "685/685 [==============================] - 32s 46ms/step - loss: 0.1529 - mse: 0.1529 - val_loss: 0.1556 - val_mse: 0.1556\n",
      "Epoch 58/1000\n",
      "685/685 [==============================] - 31s 45ms/step - loss: 0.1529 - mse: 0.1529 - val_loss: 0.1556 - val_mse: 0.1556\n",
      "Epoch 59/1000\n",
      "685/685 [==============================] - 32s 47ms/step - loss: 0.1529 - mse: 0.1529 - val_loss: 0.1551 - val_mse: 0.1551\n",
      "Epoch 60/1000\n",
      "685/685 [==============================] - 42s 61ms/step - loss: 0.1529 - mse: 0.1529 - val_loss: 0.1559 - val_mse: 0.1559\n",
      "Epoch 61/1000\n",
      "685/685 [==============================] - 31s 46ms/step - loss: 0.1528 - mse: 0.1528 - val_loss: 0.1558 - val_mse: 0.1558\n",
      "Epoch 62/1000\n",
      "685/685 [==============================] - 32s 47ms/step - loss: 0.1528 - mse: 0.1528 - val_loss: 0.1549 - val_mse: 0.1549\n",
      "Epoch 63/1000\n",
      "685/685 [==============================] - 32s 47ms/step - loss: 0.1528 - mse: 0.1528 - val_loss: 0.1554 - val_mse: 0.1554\n",
      "Epoch 64/1000\n",
      "685/685 [==============================] - 32s 47ms/step - loss: 0.1528 - mse: 0.1528 - val_loss: 0.1550 - val_mse: 0.1550\n",
      "Epoch 65/1000\n",
      "685/685 [==============================] - 32s 46ms/step - loss: 0.1527 - mse: 0.1527 - val_loss: 0.1552 - val_mse: 0.1552\n",
      "Epoch 66/1000\n",
      "685/685 [==============================] - 33s 48ms/step - loss: 0.1528 - mse: 0.1528 - val_loss: 0.1559 - val_mse: 0.1559\n",
      "Epoch 67/1000\n",
      "685/685 [==============================] - 33s 48ms/step - loss: 0.1528 - mse: 0.1528 - val_loss: 0.1557 - val_mse: 0.1557\n",
      "Epoch 68/1000\n",
      "685/685 [==============================] - 32s 46ms/step - loss: 0.1527 - mse: 0.1527 - val_loss: 0.1558 - val_mse: 0.1558\n",
      "Epoch 69/1000\n",
      "685/685 [==============================] - 32s 47ms/step - loss: 0.1526 - mse: 0.1526 - val_loss: 0.1555 - val_mse: 0.1555\n",
      "Epoch 70/1000\n",
      "685/685 [==============================] - 32s 47ms/step - loss: 0.1527 - mse: 0.1527 - val_loss: 0.1558 - val_mse: 0.1558\n",
      "Epoch 71/1000\n",
      "685/685 [==============================] - 32s 47ms/step - loss: 0.1527 - mse: 0.1527 - val_loss: 0.1549 - val_mse: 0.1549\n",
      "Epoch 72/1000\n",
      "685/685 [==============================] - 33s 48ms/step - loss: 0.1526 - mse: 0.1526 - val_loss: 0.1551 - val_mse: 0.1551\n"
     ]
    }
   ],
   "source": [
    "# Assign checkpoint paths\n",
    "lstm_ckpt_path = join(\n",
    "    config['EXP_DIR'], \"lstm_checkpoint\", \"ckpt\")\n",
    "    \n",
    "# Callbacks for LSTM\n",
    "early_stopping_callback = EarlyStopping(monitor='val_loss',\n",
    "                                        patience=config['PATIENCE'],\n",
    "                                        mode='min',\n",
    "                                        restore_best_weights=True)\n",
    "checkpoint_callback = ModelCheckpoint(lstm_ckpt_path,\n",
    "                                      monitor='val_loss',\n",
    "                                      verbose=0,\n",
    "                                      mode='min',\n",
    "                                      save_best_only=True,\n",
    "                                      save_weights_only=True)\n",
    "\n",
    "if not os.path.isfile(lstm_ckpt_path + \".index\"):\n",
    "    # Train LSTM\n",
    "    lstm_history = lstm_model.fit(embed_gen.x_train, embed_gen.y_train,\n",
    "                                  validation_split=0.05,\n",
    "                                  batch_size=config['BATCH_SIZE'],\n",
    "                                  epochs=config['EPOCH'],\n",
    "                                  callbacks=[\n",
    "                                      early_stopping_callback, checkpoint_callback],\n",
    "                                  verbose=1)\n",
    "\n",
    "    # Export model history\n",
    "    export_history(lstm_history, join(\n",
    "        config['EXP_DIR'], data.id + \"_lstm_history.csv\"))\n",
    "\n",
    "    # Plot loss curve\n",
    "    # print(\"\\nLSTM Loss Curve\")\n",
    "    # print(\"===============\", end=\"\\n\")\n",
    "    plot.loss_curve(config, lstm_history, ref=data.id + \"_LSTM\", save_plot=True,\n",
    "                    close_plot=True)\n",
    "else:\n",
    "    lstm_model.load_weights(lstm_ckpt_path)\n",
    "    print(\"LSTM model weights loaded from:\")\n",
    "    print(lstm_ckpt_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1e2cbd4b1aeee473df5f056e99f8e0261930fcf2a79deddacd0b55d462463450"
  },
  "kernelspec": {
   "display_name": "Python 3.9.9 ('venv_dl': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
